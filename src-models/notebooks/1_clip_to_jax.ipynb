{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f99f91",
   "metadata": {},
   "source": [
    "# Extract CLIP Weights for JAX\n",
    "\n",
    "Convert PyTorch CLIP (ViT-B/32) to numpy arrays for JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bec485",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e837af",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision\n",
    "\n",
    "%pip install -q git+https://github.com/openai/CLIP.git\n",
    "\n",
    "%pip install -q pillow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a70a8",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folders = [\n",
    "    '/content/drive/MyDrive/hope-models/checkpoints',\n",
    "    '/content/drive/MyDrive/hope-models/exports'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ce17a",
   "metadata": {},
   "source": [
    "## 3. Load CLIP Models\n",
    "\n",
    "Loading ViT-B/32 (base) and ViT-L/14 (large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nLoading CLIP ViT-B/32...\")\n",
    "\n",
    "clip_base, preprocess_base = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_base.eval()\n",
    "print(\"ViT-B/32 loaded\")\n",
    "\n",
    "print(\"\\nLoading CLIP ViT-L/14...\")\n",
    "try:\n",
    "    clip_large, preprocess_large = clip.load(\"ViT-L/14\", device=device)\n",
    "    clip_large.eval()\n",
    "    print(\"ViT-L/14 loaded\")\n",
    "    has_large = True\n",
    "except Exception as e:\n",
    "    print(f\"ViT-L/14 not available: {e}\")\n",
    "    has_large = False\n",
    "\n",
    "for param in clip_base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if has_large:\n",
    "    for param in clip_large.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ff38a",
   "metadata": {},
   "source": [
    "## 4. Extract Weights\n",
    "\n",
    "Convert PyTorch tensors to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_weights(model, name):\n",
    "    weights = {}\n",
    "    for param_name, param in model.named_parameters():\n",
    "        weights[param_name] = param.detach().cpu().numpy()\n",
    "    print(f\"Extracted {len(weights)} layers from {name}\")\n",
    "    return weights\n",
    "\n",
    "base_weights = extract_weights(clip_base.visual, \"ViT-B/32\")\n",
    "\n",
    "if has_large:\n",
    "    large_weights = extract_weights(clip_large.visual, \"ViT-L/14\")\n",
    "else:\n",
    "    large_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffd418",
   "metadata": {},
   "source": [
    "## 5. CLIP Constants\n",
    "\n",
    "Normalization parameters and prompts from original Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27640cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MEAN = np.array([0.48145466, 0.4578275, 0.40821073], dtype=np.float32)\n",
    "CLIP_STD = np.array([0.26862954, 0.26130258, 0.27577711], dtype=np.float32)\n",
    "CLIP_INPUT_SIZE = 224\n",
    "\n",
    "CHAOS_PROMPTS = [\n",
    "    \"completely destroyed corrupted visual data\",\n",
    "    \"incomprehensible chaotic noise patterns\",\n",
    "    \"severely distorted unrecognizable imagery\",\n",
    "    \"broken fragmented visual information\",\n",
    "    \"extreme digital corruption and artifacts\",\n",
    "    \"meaningless random pixel arrangements\",\n",
    "    \"visual chaos with no coherent structure\",\n",
    "    \"utterly corrupted incomprehensible forms\",\n",
    "]\n",
    "\n",
    "NORMAL_PROMPTS = [\n",
    "    \"a clear image\",\n",
    "    \"a recognizable picture\",\n",
    "    \"coherent visual content\",\n",
    "]\n",
    "\n",
    "print(f\"Input size: {CLIP_INPUT_SIZE}x{CLIP_INPUT_SIZE}\")\n",
    "print(f\"Mean: {CLIP_MEAN}\")\n",
    "print(f\"Std: {CLIP_STD}\")\n",
    "print(f\"Chaos prompts: {len(CHAOS_PROMPTS)}\")\n",
    "print(f\"Normal prompts: {len(NORMAL_PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7dbf3a",
   "metadata": {},
   "source": [
    "## 6. Extract Text Embeddings\n",
    "\n",
    "Pre-compute embeddings for chaos and normal prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaos_tokens = clip.tokenize(CHAOS_PROMPTS).to(device)\n",
    "normal_tokens = clip.tokenize(NORMAL_PROMPTS).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    chaos_base = clip_base.encode_text(chaos_tokens)\n",
    "    chaos_base = chaos_base / chaos_base.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    normal_base = clip_base.encode_text(normal_tokens)\n",
    "    normal_base = normal_base / normal_base.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    if has_large:\n",
    "        chaos_large = clip_large.encode_text(chaos_tokens)\n",
    "        chaos_large = chaos_large / chaos_large.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        normal_large = clip_large.encode_text(normal_tokens)\n",
    "        normal_large = normal_large / normal_large.norm(dim=-1, keepdim=True)\n",
    "    else:\n",
    "        chaos_large = None\n",
    "        normal_large = None\n",
    "\n",
    "print(f\"Chaos embeddings (base): {chaos_base.shape}\")\n",
    "print(f\"Normal embeddings (base): {normal_base.shape}\")\n",
    "\n",
    "if has_large:\n",
    "    print(f\"Chaos embeddings (large): {chaos_large.shape}\")\n",
    "    print(f\"Normal embeddings (large): {normal_large.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa7dde",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## 7. Save to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_path = '/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl'\n",
    "\n",
    "data = {\n",
    "    'base_weights': base_weights,\n",
    "    'large_weights': large_weights,\n",
    "    'chaos_embeddings_base': chaos_base.cpu().numpy(),\n",
    "    'normal_embeddings_base': normal_base.cpu().numpy(),\n",
    "    'chaos_embeddings_large': chaos_large.cpu().numpy() if has_large else None,\n",
    "    'normal_embeddings_large': normal_large.cpu().numpy() if has_large else None,\n",
    "    'clip_mean': CLIP_MEAN,\n",
    "    'clip_std': CLIP_STD,\n",
    "    'clip_input_size': CLIP_INPUT_SIZE,\n",
    "    'chaos_prompts': CHAOS_PROMPTS,\n",
    "    'normal_prompts': NORMAL_PROMPTS,\n",
    "}\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "file_size = os.path.getsize(save_path) / (1024**2)\n",
    "print(f\"Saved to: {save_path}\")\n",
    "print(f\"Size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409886cf",
   "metadata": {},
   "source": [
    "## 8. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "\n",
    "print(\"Loaded successfully\\n\")\n",
    "\n",
    "for key, value in loaded.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{key}: shape {value.shape}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"{key}: {len(value)} layers\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key}: {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d5794",
   "metadata": {},
   "source": [
    "## Complete\n",
    "\n",
    "**Extracted:**\n",
    "- ViT-B/32 weights\n",
    "- ViT-L/14 weights (if available)\n",
    "- Chaos/normal text embeddings\n",
    "- CLIP constants\n",
    "\n",
    "**Saved:** `/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl`\n",
    "\n",
    "**Next:** Run `2_noise_algorithm.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
