{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f99f91",
   "metadata": {},
   "source": [
    "# Extract CLIP Weights for JAX\n",
    "\n",
    "Convert PyTorch CLIP (ViT-B/32) to numpy arrays for JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bec485",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e837af",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision\n",
    "\n",
    "%pip install -q git+https://github.com/openai/CLIP.git\n",
    "\n",
    "%pip install -q numpy pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a70a8",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folders = [\n",
    "    '/content/drive/MyDrive/hope-models/checkpoints',\n",
    "    '/content/drive/MyDrive/hope-models/exports'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c795c",
   "metadata": {},
   "source": [
    "## 3. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ce17a",
   "metadata": {},
   "source": [
    "## 4. Load CLIP Models\n",
    "\n",
    "Loading ViT-B/32 (base) and ViT-L/14 (large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nLoading CLIP ViT-B/32...\")\n",
    "\n",
    "clip_base, preprocess_base = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_base.eval()\n",
    "print(\"ViT-B/32 loaded\")\n",
    "\n",
    "print(\"\\nLoading CLIP ViT-L/14...\")\n",
    "try:\n",
    "    clip_large, preprocess_large = clip.load(\"ViT-L/14\", device=device)\n",
    "    clip_large.eval()\n",
    "    print(\"ViT-L/14 loaded\")\n",
    "    has_large = True\n",
    "except Exception as e:\n",
    "    print(f\"ViT-L/14 not available: {e}\")\n",
    "    has_large = False\n",
    "\n",
    "for param in clip_base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if has_large:\n",
    "    for param in clip_large.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ff38a",
   "metadata": {},
   "source": [
    "## 5. Extract Weights\n",
    "\n",
    "Convert PyTorch tensors to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(model, name):\n",
    "    weights = {}\n",
    "    for param_name, param in model.named_parameters():\n",
    "        weights[param_name] = param.detach().cpu().numpy()\n",
    "    print(f\"Extracted {len(weights)} layers from {name}\")\n",
    "    return weights\n",
    "\n",
    "base_weights = extract_weights(clip_base.visual, \"ViT-B/32\")\n",
    "\n",
    "if has_large:\n",
    "    large_weights = extract_weights(clip_large.visual, \"ViT-L/14\")\n",
    "else:\n",
    "    large_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffd418",
   "metadata": {},
   "source": [
    "## 6. CLIP Constants\n",
    "\n",
    "Normalization parameters and prompts from original Hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27640cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MEAN = np.array([0.48145466, 0.4578275, 0.40821073], dtype=np.float32)\n",
    "CLIP_STD = np.array([0.26862954, 0.26130258, 0.27577711], dtype=np.float32)\n",
    "CLIP_INPUT_SIZE = 224\n",
    "\n",
    "CHAOS_PROMPTS = [\n",
    "    \"completely destroyed corrupted visual data\",\n",
    "    \"incomprehensible chaotic noise patterns\",\n",
    "    \"severely distorted unrecognizable imagery\",\n",
    "    \"broken fragmented visual information\",\n",
    "    \"extreme digital corruption and artifacts\",\n",
    "    \"meaningless random pixel arrangements\",\n",
    "    \"visual chaos with no coherent structure\",\n",
    "    \"utterly corrupted incomprehensible forms\",\n",
    "]\n",
    "\n",
    "NORMAL_PROMPTS = [\n",
    "    \"a clear image\",\n",
    "    \"a recognizable picture\",\n",
    "    \"coherent visual content\",\n",
    "]\n",
    "\n",
    "GLAZE_STYLE_PRESETS = {\n",
    "    'Abstract': \"abstract expressionist painting with chaotic brushstrokes\",\n",
    "    'Impressionist': \"soft impressionist artwork with gentle light\",\n",
    "    'Cubist': \"geometric cubist painting with fragmented forms\",\n",
    "    'Sketch': \"rough pencil sketch with loose lines\",\n",
    "    'Watercolor': \"delicate watercolor painting with flowing colors\",\n",
    "}\n",
    "\n",
    "GLAZE_SOURCE_PROMPT = \"realistic photograph with natural lighting\"\n",
    "\n",
    "NIGHTSHADE_TARGET_PROMPTS = {\n",
    "    'Dog': \"a photo of a dog\",\n",
    "    'Cat': \"a photo of a cat\",\n",
    "    'Car': \"a photo of a car\",\n",
    "    'Landscape': \"a landscape photograph\",\n",
    "    'Person': \"a photo of a person\",\n",
    "    'Building': \"a photo of a building\",\n",
    "    'Food': \"a photo of food\",\n",
    "    'Abstract': \"abstract digital art\",\n",
    "}\n",
    "\n",
    "NIGHTSHADE_GENERIC_PROMPT = \"a clear photograph\"\n",
    "\n",
    "print(f\"Input size: {CLIP_INPUT_SIZE}x{CLIP_INPUT_SIZE}\")\n",
    "print(f\"Mean: {CLIP_MEAN}\")\n",
    "print(f\"Std: {CLIP_STD}\")\n",
    "\n",
    "print(f\"\\nNoise algorithm:\")\n",
    "print(f\"  Chaos prompts: {len(CHAOS_PROMPTS)}\")\n",
    "print(f\"  Normal prompts: {len(NORMAL_PROMPTS)}\")\n",
    "\n",
    "print(f\"\\nGlaze algorithm:\")\n",
    "print(f\"  Style presets: {list(GLAZE_STYLE_PRESETS.keys())}\")\n",
    "print(f\"  Source prompt: '{GLAZE_SOURCE_PROMPT}'\")\n",
    "\n",
    "print(f\"\\nNightshade algorithm:\")\n",
    "print(f\"  Target prompts: {list(NIGHTSHADE_TARGET_PROMPTS.keys())}\")\n",
    "print(f\"  Generic prompt: '{NIGHTSHADE_GENERIC_PROMPT}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7dbf3a",
   "metadata": {},
   "source": [
    "## 7. Extract Text Embeddings\n",
    "\n",
    "Pre-compute embeddings for chaos and normal prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting Noise algorithm embeddings...\")\n",
    "\n",
    "chaos_tokens = clip.tokenize(CHAOS_PROMPTS).to(device)\n",
    "normal_tokens = clip.tokenize(NORMAL_PROMPTS).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    chaos_base = clip_base.encode_text(chaos_tokens)\n",
    "    chaos_base = chaos_base / chaos_base.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    normal_base = clip_base.encode_text(normal_tokens)\n",
    "    normal_base = normal_base / normal_base.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    if has_large:\n",
    "        chaos_large = clip_large.encode_text(chaos_tokens)\n",
    "        chaos_large = chaos_large / chaos_large.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        normal_large = clip_large.encode_text(normal_tokens)\n",
    "        normal_large = normal_large / normal_large.norm(dim=-1, keepdim=True)\n",
    "    else:\n",
    "        chaos_large = None\n",
    "        normal_large = None\n",
    "\n",
    "print(f\"  Chaos embeddings (base): {chaos_base.shape}\")\n",
    "print(f\"  Normal embeddings (base): {normal_base.shape}\")\n",
    "if has_large:\n",
    "    print(f\"  Chaos embeddings (large): {chaos_large.shape}\")\n",
    "    print(f\"  Normal embeddings (large): {normal_large.shape}\")\n",
    "\n",
    "print(\"\\nExtracting Glaze algorithm embeddings...\")\n",
    "\n",
    "glaze_style_embeddings_base = {}\n",
    "for style_name, style_prompt in GLAZE_STYLE_PRESETS.items():\n",
    "    tokens = clip.tokenize([style_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_base.encode_text(tokens)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    glaze_style_embeddings_base[style_name] = features.cpu().numpy()[0]\n",
    "    print(f\"  {style_name}: {glaze_style_embeddings_base[style_name].shape}\")\n",
    "\n",
    "tokens = clip.tokenize([GLAZE_SOURCE_PROMPT]).to(device)\n",
    "with torch.no_grad():\n",
    "    features = clip_base.encode_text(tokens)\n",
    "    features = features / features.norm(dim=-1, keepdim=True)\n",
    "glaze_source_emb_base = features.cpu().numpy()[0]\n",
    "print(f\"  Source (realistic): {glaze_source_emb_base.shape}\")\n",
    "\n",
    "if has_large:\n",
    "    glaze_style_embeddings_large = {}\n",
    "    for style_name, style_prompt in GLAZE_STYLE_PRESETS.items():\n",
    "        tokens = clip.tokenize([style_prompt]).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = clip_large.encode_text(tokens)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        glaze_style_embeddings_large[style_name] = features.cpu().numpy()[0]\n",
    "\n",
    "    tokens = clip.tokenize([GLAZE_SOURCE_PROMPT]).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_large.encode_text(tokens)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    glaze_source_emb_large = features.cpu().numpy()[0]\n",
    "else:\n",
    "    glaze_style_embeddings_large = None\n",
    "    glaze_source_emb_large = None\n",
    "\n",
    "print(\"\\nExtracting Nightshade algorithm embeddings...\")\n",
    "\n",
    "nightshade_target_embeddings_base = {}\n",
    "for target_name, target_prompt in NIGHTSHADE_TARGET_PROMPTS.items():\n",
    "    tokens = clip.tokenize([target_prompt]).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_base.encode_text(tokens)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    nightshade_target_embeddings_base[target_name] = features.cpu().numpy()[0]\n",
    "    print(f\"  {target_name}: {nightshade_target_embeddings_base[target_name].shape}\")\n",
    "\n",
    "tokens = clip.tokenize([NIGHTSHADE_GENERIC_PROMPT]).to(device)\n",
    "with torch.no_grad():\n",
    "    features = clip_base.encode_text(tokens)\n",
    "    features = features / features.norm(dim=-1, keepdim=True)\n",
    "nightshade_generic_emb_base = features.cpu().numpy()[0]\n",
    "print(f\"  Generic: {nightshade_generic_emb_base.shape}\")\n",
    "\n",
    "if has_large:\n",
    "    nightshade_target_embeddings_large = {}\n",
    "    for target_name, target_prompt in NIGHTSHADE_TARGET_PROMPTS.items():\n",
    "        tokens = clip.tokenize([target_prompt]).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = clip_large.encode_text(tokens)\n",
    "            features = features / features.norm(dim=-1, keepdim=True)\n",
    "        nightshade_target_embeddings_large[target_name] = features.cpu().numpy()[0]\n",
    "\n",
    "    tokens = clip.tokenize([NIGHTSHADE_GENERIC_PROMPT]).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = clip_large.encode_text(tokens)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "    nightshade_generic_emb_large = features.cpu().numpy()[0]\n",
    "else:\n",
    "    nightshade_target_embeddings_large = None\n",
    "    nightshade_generic_emb_large = None\n",
    "\n",
    "print(\"\\nAll embeddings extracted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa7dde",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## 8. Save to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl'\n",
    "\n",
    "data = {\n",
    "    'base_weights': base_weights,\n",
    "    'large_weights': large_weights,\n",
    "\n",
    "    'clip_mean': CLIP_MEAN,\n",
    "    'clip_std': CLIP_STD,\n",
    "    'clip_input_size': CLIP_INPUT_SIZE,\n",
    "\n",
    "    'chaos_embeddings_base': chaos_base.cpu().numpy(),\n",
    "    'normal_embeddings_base': normal_base.cpu().numpy(),\n",
    "    'chaos_embeddings_large': chaos_large.cpu().numpy() if has_large else None,\n",
    "    'normal_embeddings_large': normal_large.cpu().numpy() if has_large else None,\n",
    "    'chaos_prompts': CHAOS_PROMPTS,\n",
    "    'normal_prompts': NORMAL_PROMPTS,\n",
    "\n",
    "    'glaze_style_embeddings_base': glaze_style_embeddings_base,\n",
    "    'glaze_style_embeddings_large': glaze_style_embeddings_large,\n",
    "    'glaze_source_emb_base': glaze_source_emb_base,\n",
    "    'glaze_source_emb_large': glaze_source_emb_large,\n",
    "    'glaze_style_presets': GLAZE_STYLE_PRESETS,\n",
    "    'glaze_source_prompt': GLAZE_SOURCE_PROMPT,\n",
    "\n",
    "    'nightshade_target_embeddings_base': nightshade_target_embeddings_base,\n",
    "    'nightshade_target_embeddings_large': nightshade_target_embeddings_large,\n",
    "    'nightshade_generic_emb_base': nightshade_generic_emb_base,\n",
    "    'nightshade_generic_emb_large': nightshade_generic_emb_large,\n",
    "    'nightshade_target_prompts': NIGHTSHADE_TARGET_PROMPTS,\n",
    "    'nightshade_generic_prompt': NIGHTSHADE_GENERIC_PROMPT,\n",
    "}\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "file_size = os.path.getsize(save_path) / (1024**2)\n",
    "print(f\"Saved to: {save_path}\")\n",
    "print(f\"Size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409886cf",
   "metadata": {},
   "source": [
    "## 9. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "\n",
    "print(\"Loaded successfully\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "categories = {\n",
    "    'Model Weights': ['base_weights', 'large_weights'],\n",
    "    'CLIP Constants': ['clip_mean', 'clip_std', 'clip_input_size'],\n",
    "    'Noise Algorithm': ['chaos_embeddings_base', 'normal_embeddings_base',\n",
    "                        'chaos_embeddings_large', 'normal_embeddings_large',\n",
    "                        'chaos_prompts', 'normal_prompts'],\n",
    "    'Glaze Algorithm': ['glaze_style_embeddings_base', 'glaze_style_embeddings_large',\n",
    "                        'glaze_source_emb_base', 'glaze_source_emb_large',\n",
    "                        'glaze_style_presets', 'glaze_source_prompt'],\n",
    "    'Nightshade Algorithm': ['nightshade_target_embeddings_base', 'nightshade_target_embeddings_large',\n",
    "                             'nightshade_generic_emb_base', 'nightshade_generic_emb_large',\n",
    "                             'nightshade_target_prompts', 'nightshade_generic_prompt'],\n",
    "}\n",
    "\n",
    "for category, keys in categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for key in keys:\n",
    "        if key not in loaded:\n",
    "            continue\n",
    "        value = loaded[key]\n",
    "        if value is None:\n",
    "            print(f\"  {key}: None\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"  {key}: shape {value.shape}\")\n",
    "        elif isinstance(value, dict):\n",
    "            if all(isinstance(v, np.ndarray) for v in value.values()):\n",
    "                print(f\"  {key}: {len(value)} embeddings\")\n",
    "                for k, v in value.items():\n",
    "                    print(f\"    - {k}: {v.shape}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {len(value)} items\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"  {key}: {len(value)} items\")\n",
    "        elif isinstance(value, str):\n",
    "            print(f\"  {key}: '{value}'\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d5794",
   "metadata": {},
   "source": [
    "## Complete\n",
    "\n",
    "**Extracted:**\n",
    "- ViT-B/32 weights\n",
    "- ViT-L/14 weights (if available)\n",
    "\n",
    "**Noise Algorithm:**\n",
    "- Chaos embeddings (8 prompts)\n",
    "- Normal embeddings (3 prompts)\n",
    "\n",
    "**Glaze Algorithm:**\n",
    "- Style embeddings (5 presets: Abstract, Impressionist, Cubist, Sketch, Watercolor)\n",
    "- Source style embedding (realistic photograph)\n",
    "\n",
    "**Nightshade Algorithm:**\n",
    "- Target embeddings (8 targets: Dog, Cat, Car, Landscape, Person, Building, Food, Abstract)\n",
    "- Generic embedding (clear photograph)\n",
    "\n",
    "**Saved:** `/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl`\n",
    "\n",
    "**Next:** Run `2_noise_algorithm.ipynb`, `3_glaze_algorithm.ipynb`, or `4_nightshade_algorithm.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
