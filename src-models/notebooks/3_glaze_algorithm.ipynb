{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380a7439",
   "metadata": {},
   "source": [
    "# Glaze Style Protection Algorithm\n",
    "\n",
    "Implements style cloaking using CLIP-guided latent space manipulation in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8f206",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de46f7",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q jax[cuda12] jaxlib\n",
    "\n",
    "%pip install -q flax optax\n",
    "\n",
    "%pip install -q pillow numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec66191",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folders = [\n",
    "    '/content/drive/MyDrive/hope-models/checkpoints',\n",
    "    '/content/drive/MyDrive/hope-models/exports'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Drive mounted and folders ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b17fb",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e47b9f",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return jnp.array(img) / 255.0\n",
    "\n",
    "def save_image(img_array, path):\n",
    "    img_array = np.clip(np.array(img_array) * 255, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img_array).save(path, quality=95)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3dc68",
   "metadata": {},
   "source": [
    "## 5. Load CLIP Data\n",
    "\n",
    "Load pre-extracted weights and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    clip_data = pickle.load(f)\n",
    "\n",
    "CLIP_MEAN = clip_data['clip_mean']\n",
    "CLIP_STD = clip_data['clip_std']\n",
    "CLIP_INPUT_SIZE = clip_data['clip_input_size']\n",
    "\n",
    "GLAZE_STYLE_PRESETS = clip_data['glaze_style_presets']\n",
    "glaze_style_embeddings_raw = clip_data['glaze_style_embeddings_base']\n",
    "glaze_source_emb_raw = clip_data['glaze_source_emb_base']\n",
    "\n",
    "style_embeddings = {name: jnp.array(emb) for name, emb in glaze_style_embeddings_raw.items()}\n",
    "source_style_emb = jnp.array(glaze_source_emb_raw)\n",
    "\n",
    "print(f\"Loaded CLIP data\")\n",
    "print(f\"Mean: {CLIP_MEAN}\")\n",
    "print(f\"Std: {CLIP_STD}\")\n",
    "print(f\"Input size: {CLIP_INPUT_SIZE}\")\n",
    "print(f\"\\nGlaze style embeddings loaded:\")\n",
    "for name, emb in style_embeddings.items():\n",
    "    print(f\"  {name}: {emb.shape}\")\n",
    "print(f\"  Source (realistic): {source_style_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81571e79",
   "metadata": {},
   "source": [
    "## 6. Glaze Algorithm Parameters\n",
    "\n",
    "Style cloaking parameters based on original Hope Glaze implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'glaze_style_presets' not in clip_data or clip_data['glaze_style_presets'] is None:\n",
    "    GLAZE_STYLE_PRESETS = {\n",
    "        'Abstract': \"abstract expressionist painting with chaotic brushstrokes\",\n",
    "        'Impressionist': \"soft impressionist artwork with gentle light\",\n",
    "        'Cubist': \"geometric cubist painting with fragmented forms\",\n",
    "        'Sketch': \"rough pencil sketch with loose lines\",\n",
    "        'Watercolor': \"delicate watercolor painting with flowing colors\"\n",
    "    }\n",
    "    print(\"Note: Using hardcoded style presets (re-run 1_clip_to_jax.ipynb to save them)\")\n",
    "else:\n",
    "    GLAZE_STYLE_PRESETS = clip_data['glaze_style_presets']\n",
    "\n",
    "if 'glaze_source_prompt' not in clip_data:\n",
    "    GLAZE_SOURCE_PROMPT = \"realistic photograph with natural lighting\"\n",
    "else:\n",
    "    GLAZE_SOURCE_PROMPT = clip_data['glaze_source_prompt']\n",
    "\n",
    "INTENSITY = 0.035\n",
    "ITERATIONS = 300\n",
    "LEARNING_RATE = 0.01\n",
    "PERCEPTUAL_WEIGHT = 1.0\n",
    "ALPHA_MULTIPLIER = 2.0\n",
    "\n",
    "PARAMETER_PRESETS = {\n",
    "    'invisible': {\n",
    "        'intensity': 0.025,\n",
    "        'iterations': 350,\n",
    "        'perceptual_weight': 1.2,\n",
    "        'description': 'Maximum invisibility, moderate protection'\n",
    "    },\n",
    "    'balanced': {\n",
    "        'intensity': 0.035,\n",
    "        'iterations': 300,\n",
    "        'perceptual_weight': 1.0,\n",
    "        'description': 'Balance between invisibility and protection'\n",
    "    },\n",
    "    'strong': {\n",
    "        'intensity': 0.05,\n",
    "        'iterations': 250,\n",
    "        'perceptual_weight': 0.6,\n",
    "        'description': 'Stronger protection, slightly visible'\n",
    "    }\n",
    "}\n",
    "\n",
    "ACTIVE_PRESET = 'balanced'\n",
    "\n",
    "preset = PARAMETER_PRESETS[ACTIVE_PRESET]\n",
    "INTENSITY = preset['intensity']\n",
    "ITERATIONS = preset['iterations']\n",
    "PERCEPTUAL_WEIGHT = preset['perceptual_weight']\n",
    "\n",
    "print(f\"Active preset: {ACTIVE_PRESET}\")\n",
    "print(f\"Description: {preset['description']}\")\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Intensity: {INTENSITY} ({int(INTENSITY * 255)}/255 per channel)\")\n",
    "print(f\"  Iterations: {ITERATIONS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Perceptual weight: {PERCEPTUAL_WEIGHT}\")\n",
    "print(f\"  Alpha multiplier: {ALPHA_MULTIPLIER}\")\n",
    "\n",
    "print(f\"\\nAvailable parameter presets: {list(PARAMETER_PRESETS.keys())}\")\n",
    "print(f\"Available style presets: {list(GLAZE_STYLE_PRESETS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17334242",
   "metadata": {},
   "source": [
    "## 7. Define JAX/Flax ViT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPAttention(nn.Module):\n",
    "    num_heads: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        d_model = x.shape[-1]\n",
    "        qkv = nn.Dense(3 * d_model, name=\"in_proj\")(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "\n",
    "        def split_heads(t):\n",
    "            return t.reshape(t.shape[0], t.shape[1], self.num_heads, -1).transpose(0, 2, 1, 3)\n",
    "\n",
    "        q, k, v = split_heads(q), split_heads(k), split_heads(v)\n",
    "        scale = (d_model // self.num_heads) ** -0.5\n",
    "        attn_weights = jax.nn.softmax((q @ k.transpose(0, 1, 3, 2)) * scale, axis=-1)\n",
    "        out = (attn_weights @ v).transpose(0, 2, 1, 3).reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        return nn.Dense(d_model, name=\"out_proj\")(out)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    width: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.width * 4, name=\"c_fc\")(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(self.width, name=\"c_proj\")(x)\n",
    "        return x\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    width: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x + CLIPAttention(self.num_heads, name=\"attn\")(nn.LayerNorm(name=\"ln_1\")(x))\n",
    "        x = x + MLP(self.width, name=\"mlp\")(nn.LayerNorm(name=\"ln_2\")(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    width: int = 768\n",
    "    layers: int = 12\n",
    "    heads: int = 12\n",
    "    patch_size: int = 32\n",
    "    output_dim: int = 512\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(self.width, (self.patch_size, self.patch_size),\n",
    "                    strides=(self.patch_size, self.patch_size),\n",
    "                    padding='VALID', use_bias=False, name=\"conv1\")(x)\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
    "        cls_token = self.param('class_embedding', lambda *args: jnp.zeros((self.width,)))\n",
    "        pos_embed = self.param('positional_embedding', lambda *args: jnp.zeros((50, self.width)))\n",
    "        x = jnp.concatenate([jnp.broadcast_to(cls_token, (x.shape[0], 1, self.width)), x], axis=1)\n",
    "        x = x + pos_embed\n",
    "        x = nn.LayerNorm(name=\"ln_pre\")(x)\n",
    "        for i in range(self.layers):\n",
    "            x = ResidualAttentionBlock(self.heads, self.width, name=f\"transformer_resblocks_{i}\")(x)\n",
    "        x = nn.LayerNorm(name=\"ln_post\")(x[:, 0, :])\n",
    "        return nn.Dense(self.output_dim, use_bias=False, name=\"proj\")(x)\n",
    "\n",
    "print(\"ViT architecture defined (CLIPAttention, MLP, ResidualAttentionBlock, VisionTransformer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb76e3",
   "metadata": {},
   "source": [
    "## 8. Weight Conversion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb336d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clip_weights(pt_weights):\n",
    "    flax_params = {}\n",
    "\n",
    "    for key, value in pt_weights.items():\n",
    "        if key.startswith('visual.'):\n",
    "            key = key[7:]\n",
    "\n",
    "        value = jnp.array(value)\n",
    "\n",
    "        if key == 'class_embedding':\n",
    "            flax_params['class_embedding'] = value\n",
    "            continue\n",
    "\n",
    "        elif key == 'positional_embedding':\n",
    "            flax_params['positional_embedding'] = value\n",
    "            continue\n",
    "\n",
    "        elif key == 'proj':\n",
    "            if value.shape == (512, 768):\n",
    "                flax_params['proj/kernel'] = value.T\n",
    "            elif value.shape == (768, 512):\n",
    "                flax_params['proj/kernel'] = value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected proj shape: {value.shape}\")\n",
    "            print(f\"proj: input shape {pt_weights['proj'].shape} -> output shape {flax_params['proj/kernel'].shape}\")\n",
    "            continue\n",
    "\n",
    "        elif key == 'conv1.weight':\n",
    "            flax_params['conv1/kernel'] = jnp.transpose(value, (2, 3, 1, 0))\n",
    "            continue\n",
    "\n",
    "        key = key.replace('transformer.resblocks.', 'transformer_resblocks_').replace('.', '/')\n",
    "\n",
    "        if 'in_proj_weight' in key:\n",
    "            flax_params[key.replace('in_proj_weight', 'in_proj/kernel')] = value.T\n",
    "\n",
    "        elif 'in_proj_bias' in key:\n",
    "            flax_params[key.replace('in_proj_bias', 'in_proj/bias')] = value\n",
    "\n",
    "        elif 'weight' in key and 'ln' in key:\n",
    "            flax_params[key.replace('weight', 'scale')] = value\n",
    "\n",
    "        elif 'bias' in key and 'ln' in key:\n",
    "            flax_params[key] = value\n",
    "\n",
    "        elif 'weight' in key:\n",
    "            flax_params[key.replace('weight', 'kernel')] = value.T\n",
    "\n",
    "        else:\n",
    "            flax_params[key] = value\n",
    "\n",
    "    nested_params = {}\n",
    "    for key, value in flax_params.items():\n",
    "        parts = key.split('/')\n",
    "        curr = nested_params\n",
    "        for p in parts[:-1]:\n",
    "            curr = curr.setdefault(p, {})\n",
    "        curr[parts[-1]] = value\n",
    "\n",
    "    return {'params': nested_params}\n",
    "\n",
    "print(\"Weight conversion function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ebe1d",
   "metadata": {},
   "source": [
    "## 9. Create Reusable CLIP Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEncoder:\n",
    "    def __init__(self, weights):\n",
    "        self.model_vit = VisionTransformer()\n",
    "        self.variables = convert_clip_weights(weights)\n",
    "        print(\"CLIP encoder initialized\")\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def encode_image(self, img):\n",
    "        img_resized = jax.image.resize(img, (CLIP_INPUT_SIZE, CLIP_INPUT_SIZE, 3), method='bilinear')\n",
    "        mean = jnp.array([0.48145466, 0.4578275, 0.40821073])\n",
    "        std = jnp.array([0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "        normalized = (img_resized - mean) / std\n",
    "        features = self.model_vit.apply(self.variables, normalized[None, ...])\n",
    "\n",
    "        return features[0] / jnp.linalg.norm(features[0])\n",
    "\n",
    "clip_encoder = CLIPEncoder(clip_data['base_weights'])\n",
    "print(\"Global CLIP encoder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72eef28",
   "metadata": {},
   "source": [
    "## 10. Style Transfer Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_image_features(img):\n",
    "    return clip_encoder.encode_image(img)\n",
    "\n",
    "@jit\n",
    "def style_transfer_loss(img, source_emb, target_emb):\n",
    "    img_features = compute_image_features(img)\n",
    "\n",
    "    sim_source = jnp.dot(img_features, source_emb)\n",
    "    sim_target = jnp.dot(img_features, target_emb)\n",
    "\n",
    "    return sim_source - sim_target\n",
    "\n",
    "print(\"Style transfer loss function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955f31b",
   "metadata": {},
   "source": [
    "## 11. Glaze Protection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc53834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glaze_protect_image(img, target_style_name, intensity, iterations, perceptual_weight=0.5):\n",
    "    target_emb = style_embeddings[target_style_name]\n",
    "    epsilon = intensity\n",
    "    alpha = epsilon / iterations * ALPHA_MULTIPLIER\n",
    "\n",
    "    print(f\"\\nApplying Glaze protection...\")\n",
    "    print(f\"Target style: {target_style_name}\")\n",
    "    print(f\"Epsilon: {epsilon:.4f} ({int(epsilon * 255)}/255 per channel)\")\n",
    "    print(f\"Alpha: {alpha:.6f}\")\n",
    "    print(f\"Iterations: {iterations}\")\n",
    "    print(f\"Perceptual weight: {perceptual_weight}\")\n",
    "    print(f\"\\nCompiling JIT (this may take a few minutes on first run)...\")\n",
    "\n",
    "    def compute_edge_weight(image):\n",
    "        gray = jnp.mean(image, axis=-1)\n",
    "\n",
    "        gx = jnp.abs(gray[1:, :] - gray[:-1, :])\n",
    "        gy = jnp.abs(gray[:, 1:] - gray[:, :-1])\n",
    "\n",
    "        gx = jnp.pad(gx, ((0, 1), (0, 0)), mode='edge')\n",
    "        gy = jnp.pad(gy, ((0, 0), (0, 1)), mode='edge')\n",
    "\n",
    "        edges = jnp.sqrt(gx**2 + gy**2)\n",
    "        edges = (edges - edges.min()) / (edges.max() - edges.min() + 1e-8)\n",
    "\n",
    "        weight = 0.3 + 0.7 * edges\n",
    "        return weight[..., None]\n",
    "\n",
    "    edge_weight = compute_edge_weight(img)\n",
    "\n",
    "    @jit\n",
    "    def compute_perceptual_loss(perturbed, original):\n",
    "        diff = perturbed - original\n",
    "        smooth_penalty = jnp.mean(diff**2 * (1.5 - edge_weight))\n",
    "        return smooth_penalty\n",
    "\n",
    "    @jit\n",
    "    def combined_loss(current_img):\n",
    "        s_loss = style_transfer_loss(current_img, source_style_emb, target_emb)\n",
    "        p_loss = compute_perceptual_loss(current_img, img)\n",
    "        return s_loss + perceptual_weight * p_loss * 100\n",
    "\n",
    "    @jit\n",
    "    def pgd_step(current_img):\n",
    "        loss_val, grads = jax.value_and_grad(combined_loss)(current_img)\n",
    "\n",
    "        weighted_grads = grads * edge_weight\n",
    "\n",
    "        next_img = current_img - alpha * jnp.sign(weighted_grads)\n",
    "        delta = jnp.clip(next_img - img, -epsilon, epsilon)\n",
    "        next_img = jnp.clip(img + delta, 0.0, 1.0)\n",
    "\n",
    "        return next_img, loss_val\n",
    "\n",
    "    current_img = img\n",
    "    current_img, initial_loss = pgd_step(current_img)\n",
    "    print(f\"JIT compilation complete!\")\n",
    "    print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "\n",
    "    losses = [float(initial_loss)]\n",
    "    for i in tqdm(range(1, iterations), desc=\"Glazing\", unit=\"iter\"):\n",
    "        current_img, loss_val = pgd_step(current_img)\n",
    "        losses.append(float(loss_val))\n",
    "\n",
    "    perturbation = current_img - img\n",
    "    max_change = float(jnp.max(jnp.abs(perturbation)))\n",
    "    avg_change = float(jnp.mean(jnp.abs(perturbation)))\n",
    "\n",
    "    print(f\"\\nGlaze protection complete!\")\n",
    "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Loss improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"Max pixel change: {max_change:.4f} ({int(max_change * 255)}/255)\")\n",
    "    print(f\"Avg pixel change: {avg_change:.4f} ({int(avg_change * 255)}/255)\")\n",
    "\n",
    "    return current_img\n",
    "\n",
    "print(\"Glaze protection function defined (PGD with style transfer + perceptual optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787cfdd",
   "metadata": {},
   "source": [
    "## 12. Upload Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcebe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Upload the image sample:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    test_image_path = list(uploaded.keys())[0]\n",
    "    try:\n",
    "        original_img = load_image(test_image_path)\n",
    "        print(f\"Uploaded: {test_image_path}\")\n",
    "        print(f\"Size: {original_img.shape}\")\n",
    "        print(f\"Range: [{original_img.min():.3f}, {original_img.max():.3f}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        original_img = None\n",
    "else:\n",
    "    print(\"No file uploaded\")\n",
    "    original_img = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c4b01",
   "metadata": {},
   "source": [
    "## 13. Run Glaze Protection\n",
    "\n",
    "Choose target style from presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8692f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_img is None:\n",
    "    print(\"No image loaded. Run Step 12 first.\")\n",
    "else:\n",
    "    TARGET_STYLE = 'Abstract'  # Options: 'Abstract', 'Impressionist', 'Cubist', 'Sketch', 'Watercolor'\n",
    "\n",
    "    print(f\"Available styles: {list(style_embeddings.keys())}\")\n",
    "    print(f\"Selected style: {TARGET_STYLE}\")\n",
    "\n",
    "    try:\n",
    "        protected_img = glaze_protect_image(\n",
    "            original_img,\n",
    "            target_style_name=TARGET_STYLE,\n",
    "            intensity=INTENSITY,\n",
    "            iterations=ITERATIONS,\n",
    "            perceptual_weight=PERCEPTUAL_WEIGHT\n",
    "        )\n",
    "\n",
    "        output_path = f'/content/drive/MyDrive/hope-models/exports/glazed_{TARGET_STYLE.lower()}.jpg'\n",
    "        save_image(protected_img, output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Protection failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e82849",
   "metadata": {},
   "source": [
    "## 14. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_img is None or protected_img is None:\n",
    "    print(\"No images to display. Run steps 12 and 13 first.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    axes[0].imshow(np.array(original_img))\n",
    "    axes[0].set_title('Original Image', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(np.array(protected_img))\n",
    "    axes[1].set_title(f'Glazed ({TARGET_STYLE}, Intensity: {INTENSITY})', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Display complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c74bc96",
   "metadata": {},
   "source": [
    "## 15. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "output_path = f'/content/drive/MyDrive/hope-models/exports/glazed_{TARGET_STYLE.lower()}.jpg'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Preparing to download: {output_path}\")\n",
    "    files.download(output_path)\n",
    "else:\n",
    "    print(\"Can't find file, run step 13 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a058a",
   "metadata": {},
   "source": [
    "## 16. Batch Protection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185947cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glaze_protect_batch(image_paths, output_dir, target_style_name, intensity=0.035, iterations=300, perceptual_weight=1.0):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {img_path}\")\n",
    "\n",
    "        img = load_image(img_path)\n",
    "\n",
    "        protected = glaze_protect_image(\n",
    "            img,\n",
    "            target_style_name=target_style_name,\n",
    "            intensity=intensity,\n",
    "            iterations=iterations,\n",
    "            perceptual_weight=perceptual_weight\n",
    "        )\n",
    "\n",
    "        basename = os.path.basename(img_path)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "        output_path = os.path.join(output_dir, f\"{name}_glazed_{target_style_name.lower()}{ext}\")\n",
    "        save_image(protected, output_path)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batch Glaze protection complete!\")\n",
    "\n",
    "print(\"Batch protection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a9555",
   "metadata": {},
   "source": [
    "## 17. Export Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4164ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_glaze_model_for_export():\n",
    "    export_data = {\n",
    "        'vit_weights': clip_data['base_weights'],\n",
    "\n",
    "        'presets': {\n",
    "            'style_embeddings': {name: np.array(emb) for name, emb in style_embeddings.items()},\n",
    "            'source_style_emb': np.array(source_style_emb),\n",
    "        },\n",
    "\n",
    "        'style_prompts': GLAZE_STYLE_PRESETS,\n",
    "        'source_prompt': GLAZE_SOURCE_PROMPT,\n",
    "\n",
    "        'constants': {\n",
    "            'clip_mean': CLIP_MEAN,\n",
    "            'clip_std': CLIP_STD,\n",
    "            'clip_input_size': CLIP_INPUT_SIZE,\n",
    "        },\n",
    "\n",
    "        'parameter_presets': PARAMETER_PRESETS,\n",
    "\n",
    "        'default_params': {\n",
    "            'intensity': INTENSITY,\n",
    "            'iterations': ITERATIONS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'perceptual_weight': PERCEPTUAL_WEIGHT,\n",
    "            'alpha_multiplier': ALPHA_MULTIPLIER,\n",
    "        },\n",
    "\n",
    "        'architecture': {\n",
    "            'model_type': 'ViT-B/32',\n",
    "            'width': 768,\n",
    "            'layers': 12,\n",
    "            'heads': 12,\n",
    "            'patch_size': 32,\n",
    "            'output_dim': 512,\n",
    "        },\n",
    "\n",
    "        'algorithm': {\n",
    "            'name': 'Glaze',\n",
    "            'type': 'style_transfer',\n",
    "            'description': 'Style cloaking using CLIP-guided latent space manipulation',\n",
    "            'available_styles': list(GLAZE_STYLE_PRESETS.keys()),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    export_path = '/content/drive/MyDrive/hope-models/exports/glaze_model_export.pkl'\n",
    "\n",
    "    with open(export_path, 'wb') as f:\n",
    "        pickle.dump(export_data, f)\n",
    "\n",
    "    file_size = os.path.getsize(export_path) / (1024**2)\n",
    "    print(f\"Glaze model export data saved\")\n",
    "    print(f\"Path: {export_path}\")\n",
    "    print(f\"Size: {file_size:.2f} MB\")\n",
    "    print(f\"Available styles: {list(GLAZE_STYLE_PRESETS.keys())}\")\n",
    "    print(f\"Parameter presets: {list(PARAMETER_PRESETS.keys())}\")\n",
    "\n",
    "    return export_path\n",
    "\n",
    "export_path = save_glaze_model_for_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518a914",
   "metadata": {},
   "source": [
    "## 18. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_glaze_model_info():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GLAZE PROTECTION MODEL INFO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nArchitecture:\")\n",
    "    print(f\"  Model: ViT-B/32\")\n",
    "    print(f\"  Parameters: ~150M\")\n",
    "    print(f\"  Input size: {CLIP_INPUT_SIZE}x{CLIP_INPUT_SIZE}\")\n",
    "    print(f\"  Feature dim: 512\")\n",
    "\n",
    "    print(\"\\nStyle Embeddings:\")\n",
    "    for name, emb in style_embeddings.items():\n",
    "        print(f\"  {name}: {emb.shape}\")\n",
    "    print(f\"  Source (realistic): {source_style_emb.shape}\")\n",
    "\n",
    "    print(\"\\nDefault Parameters:\")\n",
    "    print(f\"  Intensity: {INTENSITY} ({int(INTENSITY * 255)}/255)\")\n",
    "    print(f\"  Iterations: {ITERATIONS}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Perceptual weight: {PERCEPTUAL_WEIGHT}\")\n",
    "\n",
    "    print(\"\\nParameter Presets:\")\n",
    "    for name, preset in PARAMETER_PRESETS.items():\n",
    "        marker = \"â†’\" if name == ACTIVE_PRESET else \" \"\n",
    "        print(f\"  {marker} {name}: {preset['description']}\")\n",
    "\n",
    "    print(\"\\nCapabilities:\")\n",
    "    print(f\"  Single image protection\")\n",
    "    print(f\"  Batch processing\")\n",
    "    print(f\"  5 style presets\")\n",
    "    print(f\"  Adjustable intensity\")\n",
    "    print(f\"  GPU acceleration (JAX)\")\n",
    "    print(f\"  JIT compilation\")\n",
    "\n",
    "    print(\"\\nExport Ready:\")\n",
    "    print(f\"  ONNX export compatible\")\n",
    "    print(f\"  TFLite export compatible\")\n",
    "    print(f\"  Preset embeddings saved\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print_glaze_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb33df",
   "metadata": {},
   "source": [
    "## Glaze Algorithm Complete\n",
    "\n",
    "**Implemented:**\n",
    "- JAX-based style transfer (Pure JAX - no PyTorch/CLIP)\n",
    "- CLIP-guided style cloaking\n",
    "- PGD optimization with perceptual loss\n",
    "- Edge-aware adaptive perturbation\n",
    "- 5 style presets\n",
    "\n",
    "**Parameters:**\n",
    "- Intensity: 0.035 (adjustable)\n",
    "- Iterations: 300 (adjustable)\n",
    "- Perceptual weight: 1.0 (adjustable)\n",
    "\n",
    "**Style Presets:**\n",
    "- Abstract: Abstract expressionist\n",
    "- Impressionist: Soft impressionist\n",
    "- Cubist: Geometric cubist\n",
    "- Sketch: Pencil sketch\n",
    "- Watercolor: Delicate watercolor\n",
    "\n",
    "**Saved to:** `/content/drive/MyDrive/hope-models/exports/`\n",
    "\n",
    "**Exported:** `glaze_model_export.pkl`\n",
    "\n",
    "**Next:** Run `4_nightshade_algorithm.ipynb` or `5_export_onnx.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
