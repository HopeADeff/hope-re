{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1054f37d",
   "metadata": {},
   "source": [
    "# Export Hope Protection Algorithms to ONNX\n",
    "\n",
    "This notebook converts the pre-trained JAX models from the previous notebooks into ONNX format for deployment in the Hope Tauri application.\n",
    "\n",
    "## Data Flow\n",
    "\n",
    "```\n",
    "2_noise_algorithm.ipynb     -> noise_model_export.pkl     -> noise_algorithm.onnx\n",
    "3_glaze_algorithm.ipynb     -> glaze_model_export.pkl     -> glaze_algorithm.onnx\n",
    "4_nightshade_algorithm.ipynb -> nightshade_model_export.pkl -> nightshade_algorithm.onnx\n",
    "```\n",
    "\n",
    "## Input Files\n",
    "\n",
    "The following files must exist in Google Drive before running this notebook:\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/hope-models/exports/\n",
    "|-- noise_model_export.pkl\n",
    "|-- glaze_model_export.pkl\n",
    "|-- nightshade_model_export.pkl\n",
    "```\n",
    "\n",
    "## Output Files\n",
    "\n",
    "```\n",
    "/content/drive/MyDrive/hope-models/onnx/\n",
    "|-- noise_algorithm.onnx\n",
    "|-- glaze_algorithm.onnx\n",
    "|-- nightshade_algorithm.onnx\n",
    "|-- hope_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf090a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Run these notebooks in order before this one:\n",
    "\n",
    "1. `0_setup_colab.ipynb` - Environment setup\n",
    "2. `1_clip_to_jax.ipynb` - Extract CLIP weights and embeddings\n",
    "3. `2_noise_algorithm.ipynb` - Train and export noise model\n",
    "4. `3_glaze_algorithm.ipynb` - Train and export glaze model\n",
    "5. `4_nightshade_algorithm.ipynb` - Train and export nightshade model\n",
    "\n",
    "Each algorithm notebook saves an export file containing:\n",
    "\n",
    "- ViT weights (PyTorch format)\n",
    "- Pre-computed embeddings\n",
    "- Algorithm parameters\n",
    "- Architecture configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357e578",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install JAX with CUDA 12 support for GPU acceleration, Flax for neural network definitions, jax2onnx for ONNX conversion, and onnxruntime-gpu for validation on the T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f6af6",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q --upgrade jax[cuda12] jaxlib\n",
    "\n",
    "%pip install -q flax>=0.12.2\n",
    "\n",
    "%pip install -q jax2onnx>=0.11.2 onnx onnxsim\n",
    "\n",
    "%pip install -q onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8a1c6",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Connect to Google Drive to access the exported model files and save ONNX outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "EXPORT_DIR = '/content/drive/MyDrive/hope-models/exports'\n",
    "RAW_ONNX_DIR = os.path.join(EXPORT_DIR, 'onnx-raw')\n",
    "ONNX_DIR = '/content/drive/MyDrive/hope-models/onnx'\n",
    "\n",
    "os.makedirs(RAW_ONNX_DIR, exist_ok=True)\n",
    "os.makedirs(ONNX_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Export directory: {EXPORT_DIR}\")\n",
    "print(f\"Raw ONNX directory: {RAW_ONNX_DIR}\")\n",
    "print(f\"ONNX output directory: {ONNX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdee7af",
   "metadata": {},
   "source": [
    "## 3. Import Libraries\n",
    "\n",
    "Import JAX for array operations, Flax Linen for model architecture, and ONNX tools for export and validation.\n",
    "\n",
    "Note: `jax_enable_x64` is enabled to allow float64 operations required by jax2onnx during ONNX conversion. The float64 truncation warnings from jax2onnx's internal plugin validation are harmless and suppressed via `warnings.filterwarnings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Explicitly requested dtype float64\", category=UserWarning)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from flax import linen as nn\n",
    "from typing import Dict, Any, Tuple, Callable\n",
    "\n",
    "from jax2onnx import to_onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163745b",
   "metadata": {},
   "source": [
    "## 4. Verify GPU Availability\n",
    "\n",
    "Confirm that both JAX and ONNX Runtime have access to the T4 GPU for accelerated computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"JAX Devices:\")\n",
    "for device in jax.devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "print(f\"\\nJAX Backend: {jax.default_backend()}\")\n",
    "\n",
    "print(\"\\nONNX Runtime Providers:\")\n",
    "available_providers = ort.get_available_providers()\n",
    "for provider in available_providers:\n",
    "    print(f\"  {provider}\")\n",
    "\n",
    "if 'CUDAExecutionProvider' in available_providers:\n",
    "    print(\"\\nGPU acceleration: ENABLED\")\n",
    "else:\n",
    "    print(\"\\nGPU acceleration: NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1938a",
   "metadata": {},
   "source": [
    "## 5. Verify Export Files\n",
    "\n",
    "Check that all required export files from previous notebooks exist before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_files = [\n",
    "    'noise_model_export.pkl',\n",
    "    'glaze_model_export.pkl',\n",
    "    'nightshade_model_export.pkl'\n",
    "]\n",
    "\n",
    "print(\"Checking export files:\")\n",
    "all_exist = True\n",
    "\n",
    "for filename in required_files:\n",
    "    filepath = os.path.join(EXPORT_DIR, filename)\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"FOUND\" if exists else \"MISSING\"\n",
    "\n",
    "    if exists:\n",
    "        size = os.path.getsize(filepath) / (1024 ** 2)\n",
    "        print(f\"  {filename}: {status} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  {filename}: {status}\")\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    raise FileNotFoundError(\"Missing export files. Run previous notebooks first.\")\n",
    "\n",
    "print(\"\\nAll export files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d64b4b",
   "metadata": {},
   "source": [
    "## 6. Define Vision Transformer Architecture\n",
    "\n",
    "The Vision Transformer (ViT-B/32) architecture must match exactly what was used in the training notebooks. This is a functional implementation using Flax Linen.\n",
    "\n",
    "### Architecture Parameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| hidden_dim | 768 | Transformer hidden dimension |\n",
    "| num_layers | 12 | Number of transformer blocks |\n",
    "| num_heads | 12 | Number of attention heads |\n",
    "| patch_size | 32 | Size of image patches |\n",
    "| output_dim | 512 | Final embedding dimension |\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "The scaled dot-product attention is computed as:\n",
    "\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
    "\n",
    "Where $d_k = \\frac{768}{12} = 64$ is the dimension per attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe82bf7",
   "metadata": {},
   "source": [
    "### 6.1 Multi-Head Attention\n",
    "\n",
    "The multi-head attention splits the input into multiple heads, applies attention independently, and concatenates the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726701cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    num_heads: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        head_dim = d_model // self.num_heads\n",
    "\n",
    "        qkv = nn.Dense(3 * d_model, name=\"in_proj\")(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, head_dim)\n",
    "        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scale = head_dim ** -0.5\n",
    "        attn = jnp.einsum('bhqd,bhkd->bhqk', q, k) * scale\n",
    "        attn = jax.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        out = jnp.einsum('bhqk,bhkd->bhqd', attn, v)\n",
    "        out = jnp.transpose(out, (0, 2, 1, 3))\n",
    "        out = out.reshape(batch_size, seq_len, d_model)\n",
    "\n",
    "        return nn.Dense(d_model, name=\"out_proj\")(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b89637",
   "metadata": {},
   "source": [
    "### 6.2 Feed-Forward Network\n",
    "\n",
    "The feed-forward network expands the dimension by 4x, applies GELU activation, then projects back.\n",
    "\n",
    "$$\\text{FFN}(\\mathbf{x}) = \\text{GELU}(\\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    hidden_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = nn.Dense(self.hidden_dim * 4, name=\"c_fc\")(x)\n",
     "        x = nn.gelu(x)\n",
    "        x = nn.Dense(self.hidden_dim, name=\"c_proj\")(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c042d0",
   "metadata": {},
   "source": [
    "### 6.3 Transformer Block\n",
    "\n",
    "Each transformer block applies layer normalization, multi-head attention, and feed-forward network with residual connections.\n",
    "\n",
    "$$\\mathbf{x}' = \\mathbf{x} + \\text{Attention}(\\text{LN}(\\mathbf{x}))$$\n",
    "\n",
    "$$\\mathbf{x}'' = \\mathbf{x}' + \\text{FFN}(\\text{LN}(\\mathbf{x}'))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    hidden_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = x + MultiHeadAttention(self.num_heads, name=\"attn\")(nn.LayerNorm(name=\"ln_1\")(x))\n",
    "        x = x + FeedForward(self.hidden_dim, name=\"mlp\")(nn.LayerNorm(name=\"ln_2\")(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342a8e8",
   "metadata": {},
   "source": [
    "### 6.4 Vision Transformer\n",
    "\n",
    "The complete ViT model processes images through patch embedding, transformer blocks, and final projection.\n",
    "\n",
    "For a 224x224 input with patch size 32:\n",
    "\n",
    "$$n_{patches} = \\left\\lfloor\\frac{224}{32}\\right\\rfloor^2 = 7^2 = 49$$\n",
    "\n",
    "The sequence length is $n_{patches} + 1 = 50$ (including the prepended class token).\n",
    "\n",
    "### Forward Pass Pipeline\n",
    "\n",
    "$$\\mathbf{x} \\in \\mathbb{R}^{1 \\times 224 \\times 224 \\times 3} \\xrightarrow{\\text{Conv}_{32 \\times 32}} \\mathbb{R}^{1 \\times 49 \\times 768} \\xrightarrow{[\\texttt{CLS}; \\cdot] + \\mathbf{E}_{pos}} \\mathbb{R}^{1 \\times 50 \\times 768} \\xrightarrow{\\text{Transformer} \\times 12} \\mathbb{R}^{1 \\times 50 \\times 768} \\xrightarrow{\\texttt{CLS} \\to \\text{proj}} \\mathbb{R}^{1 \\times 512}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    hidden_dim: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    patch_size: int = 32\n",
    "    output_dim: int = 512\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = nn.Conv(\n",
    "            self.hidden_dim,\n",
    "            kernel_size=(self.patch_size, self.patch_size),\n",
    "            strides=(self.patch_size, self.patch_size),\n",
    "            padding='VALID',\n",
    "            use_bias=False,\n",
    "            name=\"conv1\"\n",
    "        )(x)\n",
    "\n",
    "        x = x.reshape(batch_size, -1, self.hidden_dim)\n",
    "        num_patches = x.shape[1]\n",
    "\n",
    "        class_embedding = self.param(\n",
    "            'class_embedding',\n",
    "            nn.initializers.zeros,\n",
    "            (self.hidden_dim,)\n",
    "        )\n",
    "        class_tokens = jnp.broadcast_to(\n",
    "            class_embedding[None, None, :],\n",
    "            (batch_size, 1, self.hidden_dim)\n",
    "        )\n",
    "        x = jnp.concatenate([class_tokens, x], axis=1)\n",
    "\n",
    "        positional_embedding = self.param(\n",
    "            'positional_embedding',\n",
    "            nn.initializers.zeros,\n",
    "            (num_patches + 1, self.hidden_dim)\n",
    "        )\n",
    "        x = x + positional_embedding[None, :, :]\n",
    "\n",
    "        x = nn.LayerNorm(name=\"ln_pre\")(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = TransformerBlock(\n",
    "                self.num_heads,\n",
    "                self.hidden_dim,\n",
    "                name=f\"transformer_resblocks_{i}\"\n",
    "            )(x)\n",
    "\n",
    "        x = nn.LayerNorm(name=\"ln_post\")(x[:, 0, :])\n",
    "        x = nn.Dense(self.output_dim, use_bias=False, name=\"proj\")(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f436d",
   "metadata": {},
   "source": [
    "## 7. Weight Conversion Function\n",
    "\n",
    "Convert PyTorch CLIP weights to Flax parameter format. The export files store weights in PyTorch format, which requires transformation for use with Flax.\n",
    "\n",
    "### Transformation Rules\n",
    "\n",
    "| PyTorch Format | Flax Format | Transformation |\n",
    "|----------------|-------------|----------------|\n",
    "| Linear weight | kernel | Transpose |\n",
    "| Conv weight | kernel | Permute (2,3,1,0) |\n",
    "| LayerNorm weight | scale | Direct copy |\n",
    "| bias | bias | Direct copy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab82330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_key(key: str) -> str:\n",
    "    if key.startswith('visual.'):\n",
    "        key = key[7:]\n",
    "    key = key.replace('transformer.resblocks.', 'transformer_resblocks_')\n",
    "    key = key.replace('.', '/')\n",
    "    return key\n",
    "\n",
    "\n",
    "def transform_param(key: str, value: np.ndarray) -> Tuple[str, jnp.ndarray]:\n",
    "    value = jnp.array(value)\n",
    "\n",
    "    if key == 'class_embedding':\n",
    "        return 'class_embedding', value\n",
    "\n",
    "    if key == 'positional_embedding':\n",
    "        return 'positional_embedding', value\n",
    "\n",
    "    if key == 'proj':\n",
    "        if value.shape[0] == 512:\n",
    "            return 'proj/kernel', value.T\n",
    "        return 'proj/kernel', value\n",
    "\n",
    "    if key == 'conv1.weight':\n",
    "        return 'conv1/kernel', jnp.transpose(value, (2, 3, 1, 0))\n",
    "\n",
    "    key = flatten_key(key)\n",
    "\n",
    "    if 'in_proj_weight' in key:\n",
    "        return key.replace('in_proj_weight', 'in_proj/kernel'), value.T\n",
    "\n",
    "    if 'in_proj_bias' in key:\n",
    "        return key.replace('in_proj_bias', 'in_proj/bias'), value\n",
    "\n",
    "    if 'out_proj/weight' in key:\n",
    "        return key.replace('out_proj/weight', 'out_proj/kernel'), value.T\n",
    "\n",
    "    if 'out_proj/bias' in key:\n",
    "        return key.replace('out_proj/bias', 'out_proj/bias'), value\n",
    "\n",
    "    if 'ln_1/weight' in key:\n",
    "        return key.replace('ln_1/weight', 'ln_1/scale'), value\n",
    "\n",
    "    if 'ln_2/weight' in key:\n",
    "        return key.replace('ln_2/weight', 'ln_2/scale'), value\n",
    "\n",
    "    if 'ln_pre/weight' in key or key == 'ln_pre/weight':\n",
    "        return 'ln_pre/scale', value\n",
    "\n",
    "    if 'ln_pre/bias' in key or key == 'ln_pre/bias':\n",
    "        return 'ln_pre/bias', value\n",
    "\n",
    "    if 'ln_post/weight' in key or key == 'ln_post/weight':\n",
    "        return 'ln_post/scale', value\n",
    "\n",
    "    if 'ln_post/bias' in key or key == 'ln_post/bias':\n",
    "        return 'ln_post/bias', value\n",
    "\n",
    "    if 'c_fc/weight' in key:\n",
    "        return key.replace('c_fc/weight', 'c_fc/kernel'), value.T\n",
    "\n",
    "    if 'c_proj/weight' in key:\n",
    "        return key.replace('c_proj/weight', 'c_proj/kernel'), value.T\n",
    "\n",
    "    if 'weight' in key and 'ln' in key:\n",
    "        return key.replace('weight', 'scale'), value\n",
    "\n",
    "    if 'weight' in key:\n",
    "        return key.replace('weight', 'kernel'), value.T\n",
    "\n",
    "    return key, value\n",
    "\n",
    "\n",
    "def nest_params(flat_params: Dict[str, jnp.ndarray]) -> Dict[str, Any]:\n",
    "    nested = {}\n",
    "    for key, value in flat_params.items():\n",
    "        parts = key.split('/')\n",
    "        current = nested\n",
    "        for part in parts[:-1]:\n",
    "            if part not in current:\n",
    "                current[part] = {}\n",
    "            current = current[part]\n",
    "        current[parts[-1]] = value\n",
    "    return nested\n",
    "\n",
    "\n",
    "def convert_pytorch_weights(pytorch_weights: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "    flat_params = {}\n",
    "\n",
    "    for key, value in pytorch_weights.items():\n",
    "        new_key, new_value = transform_param(key, value)\n",
    "        flat_params[new_key] = new_value\n",
    "\n",
    "    nested = nest_params(flat_params)\n",
    "    return {'params': nested}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b32a3",
   "metadata": {},
   "source": [
    "### 7.1 Debug Weight Conversion\n",
    "\n",
    "Verify that the weight conversion produces the expected parameter structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_weight_structure(params: Dict[str, Any], prefix: str = \"\") -> None:\n",
    "    for key, value in params.items():\n",
    "        full_key = f\"{prefix}/{key}\" if prefix else key\n",
    "        if isinstance(value, dict):\n",
    "            debug_weight_structure(value, full_key)\n",
    "        else:\n",
    "            print(f\"  {full_key}: {value.shape}\")\n",
    "\n",
    "\n",
    "def verify_model_params(weights: Dict[str, Any]) -> bool:\n",
    "    model = VisionTransformer()\n",
    "    dummy_input = jnp.zeros((1, 224, 224, 3))\n",
    "\n",
    "    try:\n",
    "        output = model.apply(weights, dummy_input)\n",
    "        print(f\"Model forward pass successful\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Model forward pass failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae741e",
   "metadata": {},
   "source": [
    "## 8. Export File Loading\n",
    "\n",
    "Define a helper function to load and validate export files from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_export_file(filename: str) -> Dict[str, Any]:\n",
    "    filepath = os.path.join(EXPORT_DIR, filename)\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded: {filename}\")\n",
    "    print(f\"  Keys: {list(data.keys())}\")\n",
    "\n",
    "    if 'constants' in data:\n",
    "        print(f\"  Input size: {data['constants']['clip_input_size']}\")\n",
    "\n",
    "    if 'default_params' in data:\n",
    "        print(f\"  Parameters: {data['default_params']}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d598f9",
   "metadata": {},
   "source": [
    "## 9. Noise Algorithm Export\n",
    "\n",
    "### 9.1 Load Noise Model Export\n",
    "\n",
    "Load the noise model export file created by `2_noise_algorithm.ipynb`.\n",
    "\n",
    "### Export File Structure\n",
    "\n",
    "```python\n",
    "noise_model_export.pkl = {\n",
    "    'vit_weights': dict,\n",
    "    'presets': {\n",
    "        'noise_chaos': jnp.ndarray,    # (8, 512)\n",
    "        'noise_normal': jnp.ndarray    # (3, 512)\n",
    "    },\n",
    "    'constants': {\n",
    "        'clip_mean': np.ndarray,\n",
    "        'clip_std': np.ndarray,\n",
    "        'clip_input_size': int\n",
    "    },\n",
    "    'default_params': {\n",
    "        'intensity': float,\n",
    "        'iterations': int,\n",
    "        'alpha_multiplier': float\n",
    "    },\n",
    "    'architecture': dict\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_export = load_export_file('noise_model_export.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950c82f",
   "metadata": {},
   "source": [
    "### 9.2 Extract Noise Model Components\n",
    "\n",
    "Extract weights, embeddings, and parameters from the export file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_weights = convert_pytorch_weights(noise_export['vit_weights'])\n",
    "\n",
    "noise_constants = noise_export['constants']\n",
    "noise_mean = jnp.array(noise_constants['clip_mean'])\n",
    "noise_std = jnp.array(noise_constants['clip_std'])\n",
    "noise_input_size = noise_constants['clip_input_size']\n",
    "\n",
    "chaos_embeddings = jnp.array(noise_export['presets']['noise_chaos'])\n",
    "normal_embeddings = jnp.array(noise_export['presets']['noise_normal'])\n",
    "\n",
    "noise_params = noise_export['default_params']\n",
    "\n",
    "print(f\"Input size: {noise_input_size}\")\n",
    "print(f\"Chaos embeddings shape: {chaos_embeddings.shape}\")\n",
    "print(f\"Normal embeddings shape: {normal_embeddings.shape}\")\n",
    "print(f\"Parameters: {noise_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990940f",
   "metadata": {},
   "source": [
    "### 9.2.1 Validate Noise Model Weights\n",
    "\n",
    "Verify the converted weights work with the model before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating noise model weights...\")\n",
    "print(\"\\nParameter structure:\")\n",
    "debug_weight_structure(noise_weights['params'])\n",
    "\n",
    "print(\"\\nTesting forward pass:\")\n",
    "is_valid = verify_model_params(noise_weights)\n",
    "\n",
    "if not is_valid:\n",
    "    raise ValueError(\"Weight conversion failed for noise model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f36cb8",
   "metadata": {},
   "source": [
    "### 9.3 Define Noise Loss Function\n",
    "\n",
    "The noise protection algorithm maximizes similarity to chaos embeddings while minimizing similarity to normal embeddings.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Given an L2-normalized image embedding $\\hat{\\mathbf{z}} = \\frac{\\mathbf{z}}{\\|\\mathbf{z}\\|_2}$, the loss is:\n",
    "\n",
    "$$\\mathcal{L}_{noise}(\\mathbf{x}) = -\\frac{1}{|C|}\\sum_{i=1}^{|C|} \\hat{\\mathbf{z}}^T \\mathbf{e}_i^{chaos} + \\frac{1}{|N|}\\sum_{j=1}^{|N|} \\hat{\\mathbf{z}}^T \\mathbf{e}_j^{normal}$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\mathbf{z}} \\in \\mathbb{R}^{512}$ is the L2-normalized ViT embedding of the input image\n",
    "- $\\mathbf{e}_i^{chaos} \\in \\mathbb{R}^{512}$ are chaos text embeddings ($|C| = 8$)\n",
    "- $\\mathbf{e}_j^{normal} \\in \\mathbb{R}^{512}$ are normal text embeddings ($|N| = 3$)\n",
    "\n",
    "Minimizing $\\mathcal{L}_{noise}$ simultaneously increases $\\hat{\\mathbf{z}}^T \\mathbf{e}_i^{chaos}$ (moves toward chaotic semantics) and decreases $\\hat{\\mathbf{z}}^T \\mathbf{e}_j^{normal}$ (moves away from normal semantics).\n",
    "\n",
    "### CLIP Preprocessing\n",
    "\n",
    "Images are normalized using CLIP statistics before passing through the ViT:\n",
    "\n",
    "$$\\mathbf{x}_{norm} = \\frac{\\mathbf{x} - \\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}$$\n",
    "\n",
    "Where $\\boldsymbol{\\mu} = (0.48145466, 0.4578275, 0.40821073)$ and $\\boldsymbol{\\sigma} = (0.26862954, 0.26130258, 0.27577711)$.\n",
    "\n",
    "This normalization is baked into each ONNX model, so the Rust application only needs to provide pixel values in $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c709ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise_loss_fn(\n",
    "    params: Dict[str, Any],\n",
    "    mean: jnp.ndarray,\n",
    "    std: jnp.ndarray,\n",
    "    chaos_emb: jnp.ndarray,\n",
    "    normal_emb: jnp.ndarray\n",
    ") -> Callable:\n",
    "\n",
    "    model = VisionTransformer()\n",
    "\n",
    "    def loss_fn(image: jnp.ndarray) -> jnp.ndarray:\n",
    "        normalized = (image - mean) / std\n",
    "        features = model.apply(params, normalized)\n",
    "\n",
    "        features = features / (jnp.linalg.norm(features, axis=-1, keepdims=True) + 1e-8)\n",
    "\n",
    "        sim_chaos = jnp.mean(jnp.matmul(features, chaos_emb.T))\n",
    "        sim_normal = jnp.mean(jnp.matmul(features, normal_emb.T))\n",
    "\n",
    "        return -sim_chaos + sim_normal\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "noise_loss_fn = create_noise_loss_fn(\n",
    "    noise_weights,\n",
    "    noise_mean,\n",
    "    noise_std,\n",
    "    chaos_embeddings,\n",
    "    normal_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7ffa6",
   "metadata": {},
   "source": [
    "### 9.4 Test Noise Loss Function\n",
    "\n",
    "Verify the loss function works correctly before ONNX export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89734092",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = jnp.ones((1, noise_input_size, noise_input_size, 3), dtype=jnp.float32) * 0.5\n",
    "test_loss = noise_loss_fn(test_image)\n",
    "\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test loss value: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939a18b",
   "metadata": {},
   "source": [
    "### 9.5 Export Noise Algorithm to ONNX\n",
    "\n",
    "Convert the noise loss function to ONNX format using jax2onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax2onnx import to_onnx\n",
    "\n",
    "try:\n",
    "    sample_input = jnp.zeros((1, noise_input_size, noise_input_size, 3), dtype=jnp.float32)\n",
    "\n",
    "    test_output = noise_loss_fn(sample_input)\n",
    "    print(f\"Pre-export test passed, loss value: {test_output}\")\n",
    "\n",
    "    noise_onnx_path = os.path.join(RAW_ONNX_DIR, 'noise_algorithm.onnx')\n",
    "\n",
    "    to_onnx(\n",
    "        noise_loss_fn,\n",
    "        [(1, noise_input_size, noise_input_size, 3)],\n",
    "        return_mode=\"file\",\n",
    "        output_path=noise_onnx_path\n",
    "    )\n",
    "\n",
    "    file_size = os.path.getsize(noise_onnx_path) / (1024 ** 2)\n",
    "    print(f\"Exported: {noise_onnx_path}\")\n",
    "    print(f\"Size: {file_size:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b1f9b",
   "metadata": {},
   "source": [
    "## 10. Glaze Algorithm Export\n",
    "\n",
    "### 10.1 Load Glaze Model Export\n",
    "\n",
    "Load the glaze model export file created by `3_glaze_algorithm.ipynb`.\n",
    "\n",
    "### Export File Structure\n",
    "\n",
    "```python\n",
    "glaze_model_export.pkl = {\n",
    "    'vit_weights': dict,\n",
    "    'presets': {\n",
    "        'style_embeddings': dict,      # {style_name: (512,)}\n",
    "        'source_style_emb': np.ndarray # (512,)\n",
    "    },\n",
    "    'style_prompts': dict,\n",
    "    'source_prompt': str,\n",
    "    'constants': dict,\n",
    "    'parameter_presets': dict,\n",
    "    'default_params': dict,\n",
    "    'architecture': dict,\n",
    "    'algorithm': dict\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc58a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glaze_export = load_export_file('glaze_model_export.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a2724",
   "metadata": {},
   "source": [
    "### 10.2 Extract Glaze Model Components\n",
    "\n",
    "Extract weights, style embeddings, and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72114b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "glaze_weights = convert_pytorch_weights(glaze_export['vit_weights'])\n",
    "\n",
    "glaze_constants = glaze_export['constants']\n",
    "glaze_mean = jnp.array(glaze_constants['clip_mean'])\n",
    "glaze_std = jnp.array(glaze_constants['clip_std'])\n",
    "glaze_input_size = glaze_constants['clip_input_size']\n",
    "\n",
    "style_emb_dict = glaze_export['presets']['style_embeddings']\n",
    "style_names = list(style_emb_dict.keys())\n",
    "style_embeddings = jnp.stack([jnp.array(style_emb_dict[name]) for name in style_names])\n",
    "source_embedding = jnp.array(glaze_export['presets']['source_style_emb'])\n",
    "\n",
    "glaze_params = glaze_export['default_params']\n",
    "glaze_presets = glaze_export.get('parameter_presets', {})\n",
    "\n",
    "print(f\"Input size: {glaze_input_size}\")\n",
    "print(f\"Style names: {style_names}\")\n",
    "print(f\"Style embeddings shape: {style_embeddings.shape}\")\n",
    "print(f\"Source embedding shape: {source_embedding.shape}\")\n",
    "print(f\"Parameters: {glaze_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61080d",
   "metadata": {},
   "source": [
    "### 10.3 Define Glaze Loss Function\n",
    "\n",
    "The Glaze algorithm performs style cloaking by pushing the image embedding away from the source (realistic photo) toward a target artistic style.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Given an L2-normalized image embedding $\\hat{\\mathbf{z}} = \\frac{\\mathbf{z}}{\\|\\mathbf{z}\\|_2}$, the loss is:\n",
    "\n",
    "$$\\mathcal{L}_{glaze}(\\mathbf{x}, i) = \\hat{\\mathbf{z}}^T \\mathbf{e}_{source} - \\hat{\\mathbf{z}}^T \\mathbf{e}_{style}^{(i)}$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\mathbf{z}} \\in \\mathbb{R}^{512}$ is the L2-normalized ViT embedding of the input image\n",
    "- $\\mathbf{e}_{source} \\in \\mathbb{R}^{512}$ is the CLIP text embedding for \"realistic photograph with natural lighting\"\n",
    "- $\\mathbf{e}_{style}^{(i)} \\in \\mathbb{R}^{512}$ is the CLIP text embedding for style $i$\n",
    "- $i \\in \\{0, 1, 2, 3, 4\\}$ is the style index\n",
    "\n",
    "Minimizing $\\mathcal{L}_{glaze}$ simultaneously decreases $\\hat{\\mathbf{z}}^T \\mathbf{e}_{source}$ (moves away from realistic photo) and increases $\\hat{\\mathbf{z}}^T \\mathbf{e}_{style}^{(i)}$ (moves toward artistic style).\n",
    "\n",
    "### Style Index Mapping\n",
    "\n",
    "| Index | Style |\n",
    "|-------|-------|\n",
    "| 0 | Abstract |\n",
    "| 1 | Impressionist |\n",
    "| 2 | Cubist |\n",
    "| 3 | Sketch |\n",
    "| 4 | Watercolor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glaze_loss_fn(\n",
    "    params: Dict[str, Any],\n",
    "    mean: jnp.ndarray,\n",
    "    std: jnp.ndarray,\n",
    "    source_emb: jnp.ndarray,\n",
    "    style_emb: jnp.ndarray\n",
    ") -> Callable:\n",
    "\n",
    "    model = VisionTransformer()\n",
    "\n",
    "    def loss_fn(image: jnp.ndarray, style_index: jnp.ndarray) -> jnp.ndarray:\n",
    "        normalized = (image - mean) / std\n",
    "        features = model.apply(params, normalized)\n",
    "\n",
    "        features = features / (jnp.linalg.norm(features, axis=-1, keepdims=True) + 1e-8)\n",
    "        features = features[0]\n",
    "\n",
    "        target_style = style_emb[style_index[0]]\n",
    "\n",
    "        sim_source = jnp.dot(features, source_emb)\n",
    "        sim_style = jnp.dot(features, target_style)\n",
    "\n",
    "        return sim_source - sim_style\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "glaze_loss_fn = create_glaze_loss_fn(\n",
    "    glaze_weights,\n",
    "    glaze_mean,\n",
    "    glaze_std,\n",
    "    source_embedding,\n",
    "    style_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530367a2",
   "metadata": {},
   "source": [
    "### 10.4 Test Glaze Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = jnp.ones((1, glaze_input_size, glaze_input_size, 3), dtype=jnp.float32) * 0.5\n",
    "test_style_idx = jnp.array([0], dtype=jnp.int32)\n",
    "test_loss = glaze_loss_fn(test_image, test_style_idx)\n",
    "\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test style: {style_names[0]}\")\n",
    "print(f\"Test loss value: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e69712",
   "metadata": {},
   "source": [
    "### 10.5 Export Glaze Algorithm to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_image = jnp.zeros((1, glaze_input_size, glaze_input_size, 3), dtype=jnp.float32)\n",
    "    sample_style_idx = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "    test_output = glaze_loss_fn(sample_image, sample_style_idx)\n",
    "    print(f\"Pre-export test passed, loss value: {test_output}\")\n",
    "\n",
    "    glaze_onnx_path = os.path.join(RAW_ONNX_DIR, 'glaze_algorithm.onnx')\n",
    "\n",
    "    to_onnx(\n",
    "        glaze_loss_fn,\n",
    "        [\n",
    "            jax.ShapeDtypeStruct((1, glaze_input_size, glaze_input_size, 3), jnp.float32),\n",
    "            jax.ShapeDtypeStruct((1,), jnp.int32)\n",
    "        ],\n",
    "        return_mode=\"file\",\n",
    "        output_path=glaze_onnx_path\n",
    "    )\n",
    "\n",
    "    file_size = os.path.getsize(glaze_onnx_path) / (1024 ** 2)\n",
    "    print(f\"Exported: {glaze_onnx_path}\")\n",
    "    print(f\"Size: {file_size:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63899d1f",
   "metadata": {},
   "source": [
    "## 11. Nightshade Algorithm Export\n",
    "\n",
    "### 11.1 Load Nightshade Model Export\n",
    "\n",
    "Load the nightshade model export file created by `4_nightshade_algorithm.ipynb`.\n",
    "\n",
    "### Export File Structure\n",
    "\n",
    "```python\n",
    "nightshade_model_export.pkl = {\n",
    "    'vit_weights': dict,\n",
    "    'presets': {\n",
    "        'target_embeddings': dict,  # {target_name: (512,)}\n",
    "        'generic_emb': np.ndarray   # (512,)\n",
    "    },\n",
    "    'target_prompts': dict,\n",
    "    'generic_prompt': str,\n",
    "    'constants': dict,\n",
    "    'parameter_presets': dict,\n",
    "    'default_params': dict,\n",
    "    'architecture': dict,\n",
    "    'algorithm': dict\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nightshade_export = load_export_file('nightshade_model_export.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7d7af",
   "metadata": {},
   "source": [
    "### 11.2 Extract Nightshade Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nightshade_weights = convert_pytorch_weights(nightshade_export['vit_weights'])\n",
    "\n",
    "nightshade_constants = nightshade_export['constants']\n",
    "nightshade_mean = jnp.array(nightshade_constants['clip_mean'])\n",
    "nightshade_std = jnp.array(nightshade_constants['clip_std'])\n",
    "nightshade_input_size = nightshade_constants['clip_input_size']\n",
    "\n",
    "target_emb_dict = nightshade_export['presets']['target_embeddings']\n",
    "target_names = list(target_emb_dict.keys())\n",
    "target_embeddings = jnp.stack([jnp.array(target_emb_dict[name]) for name in target_names])\n",
    "generic_embedding = jnp.array(nightshade_export['presets']['generic_emb'])\n",
    "\n",
    "nightshade_params = nightshade_export['default_params']\n",
    "nightshade_presets = nightshade_export.get('parameter_presets', {})\n",
    "\n",
    "print(f\"Input size: {nightshade_input_size}\")\n",
    "print(f\"Target names: {target_names}\")\n",
    "print(f\"Target embeddings shape: {target_embeddings.shape}\")\n",
    "print(f\"Generic embedding shape: {generic_embedding.shape}\")\n",
    "print(f\"Parameters: {nightshade_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45b62f",
   "metadata": {},
   "source": [
    "### 11.3 Define Nightshade Loss Function\n",
    "\n",
    "The Nightshade algorithm performs targeted data poisoning by making AI models misclassify the image as a different category.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Given an L2-normalized image embedding $\\hat{\\mathbf{z}} = \\frac{\\mathbf{z}}{\\|\\mathbf{z}\\|_2}$, the loss is:\n",
    "\n",
    "$$\\mathcal{L}_{nightshade}(\\mathbf{x}, j) = \\hat{\\mathbf{z}}^T \\mathbf{e}_{generic} - \\hat{\\mathbf{z}}^T \\mathbf{e}_{target}^{(j)}$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\mathbf{z}} \\in \\mathbb{R}^{512}$ is the L2-normalized ViT embedding of the input image\n",
    "- $\\mathbf{e}_{generic} \\in \\mathbb{R}^{512}$ is the CLIP text embedding for \"a clear photograph\"\n",
    "- $\\mathbf{e}_{target}^{(j)} \\in \\mathbb{R}^{512}$ is the CLIP text embedding for poison target $j$\n",
    "- $j \\in \\{0, 1, \\ldots, 7\\}$ is the target index\n",
    "\n",
    "Minimizing $\\mathcal{L}_{nightshade}$ simultaneously decreases $\\hat{\\mathbf{z}}^T \\mathbf{e}_{generic}$ (moves away from generic photo) and increases $\\hat{\\mathbf{z}}^T \\mathbf{e}_{target}^{(j)}$ (moves toward the poison target concept).\n",
    "\n",
    "### Target Index Mapping\n",
    "\n",
    "| Index | Target |\n",
    "|-------|--------|\n",
    "| 0 | Dog |\n",
    "| 1 | Cat |\n",
    "| 2 | Car |\n",
    "| 3 | Landscape |\n",
    "| 4 | Person |\n",
    "| 5 | Building |\n",
    "| 6 | Food |\n",
    "| 7 | Abstract |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf657f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nightshade_loss_fn(\n",
    "    params: Dict[str, Any],\n",
    "    mean: jnp.ndarray,\n",
    "    std: jnp.ndarray,\n",
    "    generic_emb: jnp.ndarray,\n",
    "    target_emb: jnp.ndarray\n",
    ") -> Callable:\n",
    "\n",
    "    model = VisionTransformer()\n",
    "\n",
    "    def loss_fn(image: jnp.ndarray, target_index: jnp.ndarray) -> jnp.ndarray:\n",
    "        normalized = (image - mean) / std\n",
    "        features = model.apply(params, normalized)\n",
    "\n",
    "        features = features / (jnp.linalg.norm(features, axis=-1, keepdims=True) + 1e-8)\n",
    "        features = features[0]\n",
    "\n",
    "        target = target_emb[target_index[0]]\n",
    "\n",
    "        sim_generic = jnp.dot(features, generic_emb)\n",
    "        sim_target = jnp.dot(features, target)\n",
    "\n",
    "        return sim_generic - sim_target\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "nightshade_loss_fn = create_nightshade_loss_fn(\n",
    "    nightshade_weights,\n",
    "    nightshade_mean,\n",
    "    nightshade_std,\n",
    "    generic_embedding,\n",
    "    target_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c9514",
   "metadata": {},
   "source": [
    "### 11.4 Test Nightshade Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = jnp.ones((1, nightshade_input_size, nightshade_input_size, 3), dtype=jnp.float32) * 0.5\n",
    "test_target_idx = jnp.array([1], dtype=jnp.int32)\n",
    "test_loss = nightshade_loss_fn(test_image, test_target_idx)\n",
    "\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test target: {target_names[1]}\")\n",
    "print(f\"Test loss value: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2228781",
   "metadata": {},
   "source": [
    "### 11.5 Export Nightshade Algorithm to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1767608",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_image = jnp.zeros((1, nightshade_input_size, nightshade_input_size, 3), dtype=jnp.float32)\n",
    "    sample_target_idx = jnp.array([0], dtype=jnp.int32)\n",
    "\n",
    "    test_output = nightshade_loss_fn(sample_image, sample_target_idx)\n",
    "    print(f\"Pre-export test passed, loss value: {test_output}\")\n",
    "\n",
    "    nightshade_onnx_path = os.path.join(RAW_ONNX_DIR, 'nightshade_algorithm.onnx')\n",
    "\n",
    "    to_onnx(\n",
    "        nightshade_loss_fn,\n",
    "        [\n",
    "            jax.ShapeDtypeStruct((1, nightshade_input_size, nightshade_input_size, 3), jnp.float32),\n",
    "            jax.ShapeDtypeStruct((1,), jnp.int32)\n",
    "        ],\n",
    "        return_mode=\"file\",\n",
    "        output_path=nightshade_onnx_path\n",
    "    )\n",
    "\n",
    "    file_size = os.path.getsize(nightshade_onnx_path) / (1024 ** 2)\n",
    "    print(f\"Exported: {nightshade_onnx_path}\")\n",
    "    print(f\"Size: {file_size:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Export failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec22f0c",
   "metadata": {},
   "source": [
    "## 12. Consolidate ONNX Models\n",
    "\n",
    "jax2onnx spills tensors larger than 1 MB to external `.data` files. Since each ViT-B/32 model is ~350 MB (well under protobuf's 2 GB limit), we re-inline all weights into a single `.onnx` file per model for simpler deployment.\n",
    "\n",
    "Additionally, jax2onnx may export some weight initializers as float16 while the graph operations expect float32, causing `MatMul` type mismatches. We fix these during consolidation.\n",
    "\n",
    "Raw exports from `onnx-raw/` are consolidated into `onnx/`, then the raw directory is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3d2f0",
   "metadata": {},
   "outputs": [],
    "source": [
     "from onnx import numpy_helper, TensorProto\n",
     "import shutil\n",
     "\n",
     "\n",
     "def fix_float16_initializers(model: onnx.ModelProto) -> int:\n",
     "    fixed = 0\n",
     "    for initializer in model.graph.initializer:\n",
     "        if initializer.data_type == TensorProto.FLOAT16:\n",
     "            arr = numpy_helper.to_array(initializer).astype(np.float32)\n",
     "            new_tensor = numpy_helper.from_array(arr, name=initializer.name)\n",
     "            initializer.CopyFrom(new_tensor)\n",
     "            fixed += 1\n",
     "\n",
     "    if fixed > 0:\n",
     "        del model.graph.value_info[:]\n",
     "\n",
     "    return fixed\n",
     "\n",
     "\n",
     "def rename_model_inputs(model: onnx.ModelProto, name_map: Dict[str, str]) -> int:\n",
     "    renamed = 0\n",
     "    for inp in model.graph.input:\n",
     "        if inp.name in name_map:\n",
     "            old_name = inp.name\n",
     "            new_name = name_map[old_name]\n",
     "            for node in model.graph.node:\n",
     "                for i, node_input in enumerate(node.input):\n",
     "                    if node_input == old_name:\n",
     "                        node.input[i] = new_name\n",
     "            inp.name = new_name\n",
     "            renamed += 1\n",
     "            print(f\"  Renamed input: {old_name} -> {new_name}\")\n",
     "    return renamed\n",
     "\n",
     "\n",
     "NOISE_INPUT_NAMES = {'in_0': 'input'}\n",
     "GLAZE_INPUT_NAMES = {'in_0': 'input', 'in_1': 'style_index'}\n",
     "NIGHTSHADE_INPUT_NAMES = {'in_0': 'input', 'in_1': 'target_index'}\n",
     "\n",
     "\n",
     "def consolidate_onnx_model(raw_path: str, output_path: str, input_names: Dict[str, str] = None) -> None:\n",
     "    model = onnx.load(raw_path, load_external_data=True)\n",
     "\n",
     "    fixed = fix_float16_initializers(model)\n",
     "    if fixed > 0:\n",
     "        print(f\"  Fixed {fixed} float16 initializers -> float32\")\n",
     "\n",
     "    if input_names:\n",
     "        rename_model_inputs(model, input_names)\n",
     "\n",
     "    onnx.save_model(model, output_path)\n",
     "\n",
     "    size = os.path.getsize(output_path) / (1024 ** 2)\n",
     "    print(f\"Consolidated: {os.path.basename(output_path)} ({size:.2f} MB)\")\n",
     "\n",
     "\n",
     "print(\"Consolidating ONNX models into\", ONNX_DIR, \"...\\n\")\n",
     "\n",
     "noise_onnx_path = os.path.join(ONNX_DIR, 'noise_algorithm.onnx')\n",
     "consolidate_onnx_model(os.path.join(RAW_ONNX_DIR, 'noise_algorithm.onnx'), noise_onnx_path, NOISE_INPUT_NAMES)\n",
     "\n",
     "glaze_onnx_path = os.path.join(ONNX_DIR, 'glaze_algorithm.onnx')\n",
     "consolidate_onnx_model(os.path.join(RAW_ONNX_DIR, 'glaze_algorithm.onnx'), glaze_onnx_path, GLAZE_INPUT_NAMES)\n",
     "\n",
     "nightshade_onnx_path = os.path.join(ONNX_DIR, 'nightshade_algorithm.onnx')\n",
     "consolidate_onnx_model(os.path.join(RAW_ONNX_DIR, 'nightshade_algorithm.onnx'), nightshade_onnx_path, NIGHTSHADE_INPUT_NAMES)\n",
     "\n",
     "shutil.rmtree(RAW_ONNX_DIR)\n",
     "print(f\"\\nCleaned up raw exports: {RAW_ONNX_DIR}\")"
    ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4f1c0",
   "metadata": {},
   "source": [
    "## 13. Simplify ONNX Models\n",
    "\n",
    "Optimize each ONNX model by folding constants and eliminating redundant operations. This reduces model size and improves inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxsim import simplify\n",
    "\n",
    "def simplify_onnx_model(model_path: str) -> None:\n",
    "    original_size = os.path.getsize(model_path) / (1024 ** 2)\n",
    "\n",
    "    model = onnx.load(model_path)\n",
    "    del model.graph.value_info[:]\n",
    "    simplified_model, check = simplify(model)\n",
    "\n",
    "    if check:\n",
    "        onnx.save(simplified_model, model_path)\n",
    "        new_size = os.path.getsize(model_path) / (1024 ** 2)\n",
    "        reduction = (1 - new_size / original_size) * 100\n",
    "        print(f\"Simplified: {os.path.basename(model_path)}\")\n",
    "        print(f\"  Size: {original_size:.2f} MB -> {new_size:.2f} MB\")\n",
    "        print(f\"  Reduction: {reduction:.1f}%\")\n",
    "    else:\n",
    "        print(f\"Simplification failed: {os.path.basename(model_path)}\")\n",
    "\n",
    "\n",
    "print(\"Simplifying ONNX models...\\n\")\n",
    "simplify_onnx_model(noise_onnx_path)\n",
    "print()\n",
    "simplify_onnx_model(glaze_onnx_path)\n",
    "print()\n",
    "simplify_onnx_model(nightshade_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33ff46",
   "metadata": {},
   "source": [
    "## 14. Validate ONNX Models\n",
    "\n",
    "Verify each exported model produces correct outputs compared to the original JAX implementation.\n",
    "\n",
    "### Validation Criteria\n",
    "\n",
    "The maximum absolute difference between JAX and ONNX outputs should be within numerical tolerance:\n",
    "\n",
    "$$|\\mathcal{L}_{jax} - \\mathcal{L}_{onnx}| < 10^{-4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286135f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_onnx_model(\n",
    "    model_path: str,\n",
    "    jax_fn: Callable,\n",
    "    sample_inputs: list\n",
    ") -> bool:\n",
    "\n",
    "    print(f\"Validating: {os.path.basename(model_path)}\")\n",
    "\n",
    "    model = onnx.load(model_path)\n",
    "    onnx.checker.check_model(model)\n",
    "    print(\"  ONNX structure check: PASSED\")\n",
    "\n",
    "    providers = [\n",
    "        ('CUDAExecutionProvider', {\n",
    "            'device_id': 0,\n",
    "            'arena_extend_strategy': 'kNextPowerOfTwo',\n",
    "            'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
    "        }),\n",
    "        'CPUExecutionProvider'\n",
    "    ]\n",
    "\n",
    "    session = ort.InferenceSession(model_path, providers=providers)\n",
    "    active_provider = session.get_providers()[0]\n",
    "    print(f\"  Execution provider: {active_provider}\")\n",
    "\n",
    "    ort_inputs = {}\n",
    "    for i, inp in enumerate(session.get_inputs()):\n",
    "        ort_inputs[inp.name] = np.array(sample_inputs[i])\n",
    "\n",
    "    onnx_output = session.run(None, ort_inputs)[0]\n",
    "    jax_output = np.array(jax_fn(*sample_inputs))\n",
    "\n",
    "    if jax_output.shape == ():\n",
    "        jax_output = jax_output.reshape(onnx_output.shape)\n",
    "\n",
    "    max_diff = np.abs(jax_output - onnx_output).max()\n",
    "    is_valid = max_diff < 1e-4\n",
    "\n",
    "    print(f\"  JAX output: {float(jax_output.flatten()[0]):.6f}\")\n",
    "    print(f\"  ONNX output: {float(onnx_output.flatten()[0]):.6f}\")\n",
    "    print(f\"  Max difference: {max_diff:.2e}\")\n",
    "    print(f\"  Validation: {'PASSED' if is_valid else 'FAILED'}\")\n",
    "\n",
    "    return is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a8e90",
   "metadata": {},
   "source": [
    "### 14.1 Validate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = jnp.ones((1, 224, 224, 3), dtype=jnp.float32) * 0.5\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ONNX MODEL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nNoise Algorithm:\")\n",
    "noise_valid = validate_onnx_model(\n",
    "    noise_onnx_path,\n",
    "    noise_loss_fn,\n",
    "    [test_image]\n",
    ")\n",
    "\n",
    "print(\"\\nGlaze Algorithm:\")\n",
    "glaze_valid = validate_onnx_model(\n",
    "    glaze_onnx_path,\n",
    "    glaze_loss_fn,\n",
    "    [test_image, jnp.array([2], dtype=jnp.int32)]\n",
    ")\n",
    "\n",
    "print(\"\\nNightshade Algorithm:\")\n",
    "nightshade_valid = validate_onnx_model(\n",
    "    nightshade_onnx_path,\n",
    "    nightshade_loss_fn,\n",
    "    [test_image, jnp.array([3], dtype=jnp.int32)]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "all_valid = noise_valid and glaze_valid and nightshade_valid\n",
    "print(f\"Overall validation: {'PASSED' if all_valid else 'FAILED'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab8bb1",
   "metadata": {},
   "source": [
    "## 15. Performance Benchmark\n",
    "\n",
    "Compare inference speed between CPU and GPU execution providers to verify GPU acceleration is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_onnx_model(model_path: str, sample_inputs: dict, num_runs: int = 100) -> None:\n",
    "    print(f\"Benchmarking: {os.path.basename(model_path)}\")\n",
    "\n",
    "    session_gpu = ort.InferenceSession(\n",
    "        model_path,\n",
    "        providers=['CUDAExecutionProvider']\n",
    "    )\n",
    "\n",
    "    for _ in range(10):\n",
    "        session_gpu.run(None, sample_inputs)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        session_gpu.run(None, sample_inputs)\n",
    "    gpu_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "\n",
    "    session_cpu = ort.InferenceSession(\n",
    "        model_path,\n",
    "        providers=['CPUExecutionProvider']\n",
    "    )\n",
    "\n",
    "    for _ in range(10):\n",
    "        session_cpu.run(None, sample_inputs)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        session_cpu.run(None, sample_inputs)\n",
    "    cpu_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "\n",
    "    speedup = cpu_time / gpu_time\n",
    "\n",
    "    print(f\"  GPU (CUDA): {gpu_time:.2f} ms/inference\")\n",
    "    print(f\"  CPU: {cpu_time:.2f} ms/inference\")\n",
    "    print(f\"  Speedup: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_np = np.ones((1, 224, 224, 3), dtype=np.float32) * 0.5\n",
    "\n",
    "noise_session = ort.InferenceSession(noise_onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "noise_input_name = noise_session.get_inputs()[0].name\n",
    "\n",
    "glaze_session = ort.InferenceSession(glaze_onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "glaze_inputs = {\n",
    "    glaze_session.get_inputs()[0].name: test_image_np,\n",
    "    glaze_session.get_inputs()[1].name: np.array([0], dtype=np.int32)\n",
    "}\n",
    "\n",
    "nightshade_session = ort.InferenceSession(nightshade_onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "nightshade_inputs = {\n",
    "    nightshade_session.get_inputs()[0].name: test_image_np,\n",
    "    nightshade_session.get_inputs()[1].name: np.array([0], dtype=np.int32)\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "benchmark_onnx_model(noise_onnx_path, {noise_input_name: test_image_np})\n",
    "print()\n",
    "benchmark_onnx_model(glaze_onnx_path, glaze_inputs)\n",
    "print()\n",
    "benchmark_onnx_model(nightshade_onnx_path, nightshade_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bcf9d3",
   "metadata": {},
   "source": [
    "## 16. Export Configuration\n",
    "\n",
    "Save algorithm parameters and metadata as a JSON configuration file for use by the Rust application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86040cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_value(value):\n",
    "    if hasattr(value, \"tolist\"):\n",
    "        return value.tolist()\n",
    "    if isinstance(value, (np.generic, jnp.ndarray)):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "\n",
    "config = {\n",
    "    'input': {\n",
    "        'size': int(noise_input_size),\n",
    "        'format': 'NHWC',\n",
    "        'dtype': 'float32',\n",
    "        'range': [0.0, 1.0],\n",
    "        'preprocessing': 'included_in_model'\n",
    "    },\n",
    "    'output': {\n",
    "        'type': 'scalar',\n",
    "        'dtype': 'float32',\n",
    "        'description': 'loss value to minimize via PGD'\n",
    "    },\n",
    "    'noise': {\n",
    "        'file': 'noise_algorithm.onnx',\n",
    "        'inputs': [\n",
    "            {'name': 'input', 'shape': [1, 224, 224, 3], 'dtype': 'float32'}\n",
    "        ],\n",
    "        'parameters': {k: serialize_value(v) for k, v in noise_params.items()}\n",
    "    },\n",
    "    'glaze': {\n",
    "        'file': 'glaze_algorithm.onnx',\n",
    "        'inputs': [\n",
    "            {'name': 'input', 'shape': [1, 224, 224, 3], 'dtype': 'float32'},\n",
    "            {'name': 'style_index', 'shape': [1], 'dtype': 'int32'}\n",
    "        ],\n",
    "        'styles': style_names,\n",
    "        'parameters': {k: serialize_value(v) for k, v in glaze_params.items()},\n",
    "        'presets': {k: {pk: serialize_value(pv) for pk, pv in v.items()} for k, v in glaze_presets.items()}\n",
    "    },\n",
    "    'nightshade': {\n",
    "        'file': 'nightshade_algorithm.onnx',\n",
    "        'inputs': [\n",
    "            {'name': 'input', 'shape': [1, 224, 224, 3], 'dtype': 'float32'},\n",
    "            {'name': 'target_index', 'shape': [1], 'dtype': 'int32'}\n",
    "        ],\n",
    "        'targets': target_names,\n",
    "        'parameters': {k: serialize_value(v) for k, v in nightshade_params.items()},\n",
    "        'presets': {k: {pk: serialize_value(pv) for pk, pv in v.items()} for k, v in nightshade_presets.items()}\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = os.path.join(ONNX_DIR, 'hope_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Configuration saved: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5398db",
   "metadata": {},
   "source": [
    "## 17. Final Verification\n",
    "\n",
    "List all exported files and verify completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72954a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "expected_files = [\n",
    "    'noise_algorithm.onnx',\n",
    "    'glaze_algorithm.onnx',\n",
    "    'nightshade_algorithm.onnx',\n",
    "    'hope_config.json'\n",
    "]\n",
    "\n",
    "total_size = 0\n",
    "\n",
    "for filename in expected_files:\n",
    "    filepath = os.path.join(ONNX_DIR, filename)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        total_size += size\n",
    "\n",
    "        if size > 1024 * 1024:\n",
    "            size_str = f\"{size / (1024 ** 2):.2f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{size / 1024:.2f} KB\"\n",
    "\n",
    "        print(f\"  {filename}: {size_str}\")\n",
    "    else:\n",
    "        print(f\"  {filename}: MISSING\")\n",
    "\n",
    "print()\n",
    "print(f\"Total size: {total_size / (1024 ** 2):.2f} MB\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Export complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e768427",
   "metadata": {},
   "source": [
    "## 18. Summary\n",
    "\n",
    "### Exported Files\n",
    "\n",
    "| File | Inputs | Output | Description |\n",
    "|------|--------|--------|-------------|\n",
    "| `noise_algorithm.onnx` | image `(1,224,224,3)` float32 | loss scalar | Chaos/normal similarity loss |\n",
    "| `glaze_algorithm.onnx` | image `(1,224,224,3)` float32, style\\_index `(1,)` int32 | loss scalar | Style cloaking loss |\n",
    "| `nightshade_algorithm.onnx` | image `(1,224,224,3)` float32, target\\_index `(1,)` int32 | loss scalar | Targeted data poisoning loss |\n",
    "| `hope_config.json` | - | - | Algorithm parameters and metadata |\n",
    "\n",
    "### Image Input Specification\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Shape | `(1, 224, 224, 3)` |\n",
    "| Format | NHWC (batch, height, width, channels) |\n",
    "| Data type | float32 |\n",
    "| Range | $[0.0, 1.0]$ |\n",
    "| Preprocessing | CLIP normalization included in model |\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "All three algorithms share the same backbone (ViT-B/32 CLIP visual encoder) and produce a scalar loss. The Rust/Tauri application minimizes these losses via PGD to generate adversarial perturbations.\n",
    "\n",
    "**Noise:**\n",
    "\n",
    "$$\\mathcal{L}_{noise} = -\\frac{1}{|C|}\\sum_{i=1}^{|C|} \\hat{\\mathbf{z}}^T \\mathbf{e}_i^{chaos} + \\frac{1}{|N|}\\sum_{j=1}^{|N|} \\hat{\\mathbf{z}}^T \\mathbf{e}_j^{normal}$$\n",
    "\n",
    "**Glaze:**\n",
    "\n",
    "$$\\mathcal{L}_{glaze} = \\hat{\\mathbf{z}}^T \\mathbf{e}_{source} - \\hat{\\mathbf{z}}^T \\mathbf{e}_{style}^{(i)}$$\n",
    "\n",
    "**Nightshade:**\n",
    "\n",
    "$$\\mathcal{L}_{nightshade} = \\hat{\\mathbf{z}}^T \\mathbf{e}_{generic} - \\hat{\\mathbf{z}}^T \\mathbf{e}_{target}^{(j)}$$\n",
    "\n",
    "### Projected Gradient Descent (PGD)\n",
    "\n",
    "The adversarial perturbation is computed iteratively:\n",
    "\n",
    "$$\\delta_{t+1} = \\Pi_{\\epsilon}\\left(\\delta_t - \\alpha \\cdot \\text{sign}(\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x} + \\delta_t))\\right)$$\n",
    "\n",
    "$$\\mathbf{x}_{adv} = \\text{clip}(\\mathbf{x} + \\delta_T, 0, 1)$$\n",
    "\n",
    "Where:\n",
    "- $\\delta_t$ is the perturbation at step $t$\n",
    "- $\\Pi_{\\epsilon}$ projects onto the $\\ell_\\infty$ ball: $\\|\\delta\\|_\\infty \\leq \\epsilon$\n",
    "- $\\epsilon$ is the intensity (perturbation budget) from configuration\n",
    "- $\\alpha = \\frac{\\epsilon \\cdot k}{T}$ is the step size, where $k$ is the alpha multiplier and $T$ is the total iterations\n",
    "- $\\text{sign}(\\cdot)$ is the element-wise sign function\n",
    "\n",
    "### Embedding Normalization\n",
    "\n",
    "Image embeddings are L2-normalized before computing cosine similarity (which reduces to dot product for unit vectors):\n",
    "\n",
    "$$\\hat{\\mathbf{z}} = \\frac{\\mathbf{z}}{\\|\\mathbf{z}\\|_2 + \\varepsilon} \\quad \\Rightarrow \\quad \\text{sim}(\\hat{\\mathbf{a}}, \\hat{\\mathbf{b}}) = \\hat{\\mathbf{a}}^T \\hat{\\mathbf{b}} = \\cos\\theta$$\n",
    "\n",
    "where $\\varepsilon = 10^{-8}$ prevents division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846e820",
   "metadata": {},
   "source": [
    "## 19. Next Steps\n",
    "\n",
    "The ONNX models are now ready for integration into the Hope Tauri application.\n",
    "\n",
    "### Files Location\n",
    "\n",
    "All exported files are saved to: `/content/drive/MyDrive/hope-models/onnx/`\n",
    "\n",
    "### Download Files\n",
    "\n",
    "To download the files for local development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ce4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Files available for download:\")\n",
    "for filename in expected_files:\n",
    "    filepath = os.path.join(ONNX_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
