{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea26dba",
   "metadata": {},
   "source": [
    "# Nightshade Data Poisoning Algorithm\n",
    "\n",
    "Implements targeted misclassification using CLIP-guided adversarial perturbations in JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74384cee",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install JAX with CUDA 12 support and required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a1d51",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q jax[cuda12] jaxlib\n",
    "\n",
    "%pip install -q flax optax\n",
    "\n",
    "%pip install -q pillow numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6191b",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## 2. Mount Google Drive\n",
    "\n",
    "Connect to Google Drive to load CLIP weights and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2753370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folders = [\n",
    "    '/content/drive/MyDrive/hope-models/checkpoints',\n",
    "    '/content/drive/MyDrive/hope-models/exports'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Drive mounted and folders ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00abba9",
   "metadata": {},
   "source": [
    "## 3. Import Libraries\n",
    "\n",
    "Load JAX, Flax, and utility libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da57f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf739fd6",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "Image loading, saving, and visualization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return jnp.array(img) / 255.0\n",
    "\n",
    "def save_image(img_array, path):\n",
    "    img_array = np.clip(np.array(img_array) * 255, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img_array).save(path, quality=95)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def get_edge_mask(img):\n",
    "    gray = jnp.mean(img, axis=-1)\n",
    "\n",
    "    gx = jnp.abs(gray[1:, :] - gray[:-1, :])\n",
    "    gy = jnp.abs(gray[:, 1:] - gray[:, :-1])\n",
    "    gx = jnp.pad(gx, ((0, 1), (0, 0)), mode='edge')\n",
    "    gy = jnp.pad(gy, ((0, 0), (0, 1)), mode='edge')\n",
    "\n",
    "    edges = jnp.sqrt(gx**2 + gy**2)\n",
    "    edges = (edges - edges.min()) / (edges.max() - edges.min() + 1e-8)\n",
    "\n",
    "    return (0.3 + 0.7 * edges)[..., None]\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93eeea6",
   "metadata": {},
   "source": [
    "## 5. Load CLIP Data\n",
    "\n",
    "Load pre-extracted CLIP weights and Nightshade target embeddings from `1_clip_to_jax.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9354ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data_path = '/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl'\n",
    "\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        with open(data_path, 'rb') as f:\n",
    "            clip_data = pickle.load(f)\n",
    "        break\n",
    "    except OSError as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"Drive connection lost. Reconnecting (attempt {attempt + 1}/{max_retries})...\")\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive', force_remount=True)\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "CLIP_MEAN = clip_data['clip_mean']\n",
    "CLIP_STD = clip_data['clip_std']\n",
    "CLIP_INPUT_SIZE = clip_data['clip_input_size']\n",
    "\n",
    "NIGHTSHADE_TARGET_PROMPTS = clip_data['nightshade_target_prompts']\n",
    "nightshade_target_embeddings_raw = clip_data['nightshade_target_embeddings_base']\n",
    "nightshade_generic_emb_raw = clip_data['nightshade_generic_emb_base']\n",
    "\n",
    "target_embeddings = {name: jnp.array(emb) for name, emb in nightshade_target_embeddings_raw.items()}\n",
    "generic_emb = jnp.array(nightshade_generic_emb_raw)\n",
    "\n",
    "print(f\"Loaded CLIP data\")\n",
    "print(f\"Mean: {CLIP_MEAN}\")\n",
    "print(f\"Std: {CLIP_STD}\")\n",
    "print(f\"Input size: {CLIP_INPUT_SIZE}\")\n",
    "print(f\"\\nNightshade target embeddings loaded:\")\n",
    "for name, emb in target_embeddings.items():\n",
    "    print(f\"  {name}: {emb.shape}\")\n",
    "print(f\"  Generic (clear photo): {generic_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b615fee",
   "metadata": {},
   "source": [
    "## 6. Nightshade Algorithm Parameters\n",
    "\n",
    "**Tuned for invisibility + maximum AI poisoning effect**\n",
    "\n",
    "Key principles:\n",
    "- **Low intensity (0.02-0.04)**: Imperceptible to humans\n",
    "- **High iterations (400-600)**: Subtle but effective changes\n",
    "- **High perceptual weight (1.5-2.0)**: Preserve visual appearance\n",
    "- **Targeted misclassification**: Make AI learn wrong class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nightshade_target_prompts' not in clip_data or clip_data['nightshade_target_prompts'] is None:\n",
    "    NIGHTSHADE_TARGET_PROMPTS = {\n",
    "        'Dog': \"a photo of a dog\",\n",
    "        'Cat': \"a photo of a cat\",\n",
    "        'Car': \"a photo of a car\",\n",
    "        'Landscape': \"a landscape photograph\",\n",
    "        'Person': \"a photo of a person\",\n",
    "        'Building': \"a photo of a building\",\n",
    "        'Food': \"a photo of food\",\n",
    "        'Abstract': \"abstract digital art\",\n",
    "    }\n",
    "    print(\"Note: Using hardcoded target prompts (re-run 1_clip_to_jax.ipynb to save them)\")\n",
    "else:\n",
    "    NIGHTSHADE_TARGET_PROMPTS = clip_data['nightshade_target_prompts']\n",
    "\n",
    "if 'nightshade_generic_prompt' not in clip_data:\n",
    "    NIGHTSHADE_GENERIC_PROMPT = \"a clear photograph\"\n",
    "else:\n",
    "    NIGHTSHADE_GENERIC_PROMPT = clip_data['nightshade_generic_prompt']\n",
    "\n",
    "INTENSITY = 0.03 # Very low - almost invisible\n",
    "ITERATIONS = 500 # High - subtle but effective\n",
    "LEARNING_RATE = 0.01\n",
    "PERCEPTUAL_WEIGHT = 1.5 # Strong - preserve human perception\n",
    "ALPHA_MULTIPLIER = 2.5\n",
    "\n",
    "PARAMETER_PRESETS = {\n",
    "    'subtle': {\n",
    "        'intensity': 0.02,\n",
    "        'iterations': 600,\n",
    "        'perceptual_weight': 2.0,\n",
    "        'description': 'Maximum invisibility, slower but more effective'\n",
    "    },\n",
    "    'balanced': {\n",
    "        'intensity': 0.03,\n",
    "        'iterations': 500,\n",
    "        'perceptual_weight': 1.5,\n",
    "        'description': 'Balance between invisibility and speed'\n",
    "    },\n",
    "    'strong': {\n",
    "        'intensity': 0.04,\n",
    "        'iterations': 400,\n",
    "        'perceptual_weight': 1.0,\n",
    "        'description': 'Stronger poisoning, slightly more visible'\n",
    "    }\n",
    "}\n",
    "\n",
    "ACTIVE_PRESET = 'balanced'\n",
    "\n",
    "preset = PARAMETER_PRESETS[ACTIVE_PRESET]\n",
    "INTENSITY = preset['intensity']\n",
    "ITERATIONS = preset['iterations']\n",
    "PERCEPTUAL_WEIGHT = preset['perceptual_weight']\n",
    "\n",
    "print(f\"Active preset: {ACTIVE_PRESET}\")\n",
    "print(f\"Description: {preset['description']}\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Intensity: {INTENSITY} ({int(INTENSITY * 255)}/255 per channel)\")\n",
    "print(f\"  Iterations: {ITERATIONS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Perceptual weight: {PERCEPTUAL_WEIGHT}\")\n",
    "print(f\"  Alpha multiplier: {ALPHA_MULTIPLIER}\")\n",
    "print(f\"\\nAvailable parameter presets: {list(PARAMETER_PRESETS.keys())}\")\n",
    "print(f\"Available target classes: {list(NIGHTSHADE_TARGET_PROMPTS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c907c1d",
   "metadata": {},
   "source": [
    "## 7. Define JAX/Flax ViT Architecture\n",
    "\n",
    "Reuse same CLIP Vision Transformer from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91469fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        d_model = x.shape[-1]\n",
    "        qkv = nn.Dense(3 * d_model, name=\"in_proj\")(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "\n",
    "        def split_heads(t):\n",
    "            return t.reshape(t.shape[0], t.shape[1], self.num_heads, -1).transpose(0, 2, 1, 3)\n",
    "\n",
    "        q, k, v = split_heads(q), split_heads(k), split_heads(v)\n",
    "        scale = (d_model // self.num_heads) ** -0.5\n",
    "        attn_weights = jax.nn.softmax((q @ k.transpose(0, 1, 3, 2)) * scale, axis=-1)\n",
    "        out = (attn_weights @ v).transpose(0, 2, 1, 3).reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        return nn.Dense(d_model, name=\"out_proj\")(out)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    width: int\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.width * 4, name=\"c_fc\")(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(self.width, name=\"c_proj\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    width: int\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x + CLIPAttention(self.num_heads, name=\"attn\")(nn.LayerNorm(name=\"ln_1\")(x))\n",
    "        x = x + MLP(self.width, name=\"mlp\")(nn.LayerNorm(name=\"ln_2\")(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    width: int = 768\n",
    "    layers: int = 12\n",
    "    heads: int = 12\n",
    "    patch_size: int = 32\n",
    "    output_dim: int = 512\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(self.width, (self.patch_size, self.patch_size),\n",
    "                    strides=(self.patch_size, self.patch_size),\n",
    "                    padding='VALID', use_bias=False, name=\"conv1\")(x)\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
    "        cls_token = self.param('class_embedding', lambda *args: jnp.zeros((self.width,)))\n",
    "        pos_embed = self.param('positional_embedding', lambda *args: jnp.zeros((50, self.width)))\n",
    "        x = jnp.concatenate([jnp.broadcast_to(cls_token, (x.shape[0], 1, self.width)), x], axis=1)\n",
    "        x = x + pos_embed\n",
    "        x = nn.LayerNorm(name=\"ln_pre\")(x)\n",
    "        for i in range(self.layers):\n",
    "            x = ResidualAttentionBlock(self.heads, self.width, name=f\"transformer_resblocks_{i}\")(x)\n",
    "        x = nn.LayerNorm(name=\"ln_post\")(x[:, 0, :])\n",
    "\n",
    "        return nn.Dense(self.output_dim, use_bias=False, name=\"proj\")(x)\n",
    "\n",
    "print(\"ViT architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339c155",
   "metadata": {},
   "source": [
    "## 8. Weight Conversion Function\n",
    "\n",
    "Convert PyTorch CLIP weights to Flax format (reused from previous notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clip_weights(pt_weights):\n",
    "    flax_params = {}\n",
    "\n",
    "    for key, value in pt_weights.items():\n",
    "        if key.startswith('visual.'):\n",
    "            key = key[7:]\n",
    "\n",
    "        value = jnp.array(value)\n",
    "\n",
    "        if key == 'class_embedding':\n",
    "            flax_params['class_embedding'] = value\n",
    "            continue\n",
    "        elif key == 'positional_embedding':\n",
    "            flax_params['positional_embedding'] = value\n",
    "            continue\n",
    "        elif key == 'proj':\n",
    "            if value.shape == (512, 768):\n",
    "                flax_params['proj/kernel'] = value.T\n",
    "            elif value.shape == (768, 512):\n",
    "                flax_params['proj/kernel'] = value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected proj shape: {value.shape}\")\n",
    "            continue\n",
    "        elif key == 'conv1.weight':\n",
    "            flax_params['conv1/kernel'] = jnp.transpose(value, (2, 3, 1, 0))\n",
    "            continue\n",
    "\n",
    "        key = key.replace('transformer.resblocks.', 'transformer_resblocks_').replace('.', '/')\n",
    "\n",
    "        if 'in_proj_weight' in key:\n",
    "            flax_params[key.replace('in_proj_weight', 'in_proj/kernel')] = value.T\n",
    "        elif 'in_proj_bias' in key:\n",
    "            flax_params[key.replace('in_proj_bias', 'in_proj/bias')] = value\n",
    "        elif 'weight' in key and 'ln' in key:\n",
    "            flax_params[key.replace('weight', 'scale')] = value\n",
    "        elif 'bias' in key and 'ln' in key:\n",
    "            flax_params[key] = value\n",
    "        elif 'weight' in key:\n",
    "            flax_params[key.replace('weight', 'kernel')] = value.T\n",
    "        else:\n",
    "            flax_params[key] = value\n",
    "\n",
    "    nested_params = {}\n",
    "    for key, value in flax_params.items():\n",
    "        parts = key.split('/')\n",
    "        curr = nested_params\n",
    "        for p in parts[:-1]:\n",
    "            curr = curr.setdefault(p, {})\n",
    "        curr[parts[-1]] = value\n",
    "\n",
    "    return {'params': nested_params}\n",
    "\n",
    "print(\"Weight conversion function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44173ec6",
   "metadata": {},
   "source": [
    "## 9. Create CLIP Encoder\n",
    "\n",
    "Reusable encoder for image → feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEncoder:\n",
    "    def __init__(self, weights):\n",
    "        self.model_vit = VisionTransformer()\n",
    "        self.variables = convert_clip_weights(weights)\n",
    "\n",
    "        print(\"CLIP encoder initialized\")\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def encode_image(self, img):\n",
    "        img_resized = jax.image.resize(img, (CLIP_INPUT_SIZE, CLIP_INPUT_SIZE, 3), method='bilinear')\n",
    "        mean = jnp.array([0.48145466, 0.4578275, 0.40821073])\n",
    "        std = jnp.array([0.26862954, 0.26130258, 0.27577711])\n",
    "        normalized = (img_resized - mean) / std\n",
    "        features = self.model_vit.apply(self.variables, normalized[None, ...])\n",
    "\n",
    "        return features[0] / jnp.linalg.norm(features[0])\n",
    "\n",
    "clip_encoder = CLIPEncoder(clip_data['base_weights'])\n",
    "print(\"Global CLIP encoder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e27c52",
   "metadata": {},
   "source": [
    "## 10. Nightshade Poisoning Loss Function\n",
    "\n",
    "**Goal:** Minimize similarity to real class, maximize similarity to target (wrong) class.\n",
    "\n",
    "**Example:**\n",
    "- Real image: Dog\n",
    "- Loss pushes AI to see it as: Cat\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{nightshade}} = \\text{sim}(\\text{img}, \\text{source}) - \\text{sim}(\\text{img}, \\text{target})$$\n",
    "\n",
    "Where:\n",
    "- `sim(img, source)` = Cosine similarity to real class (e.g., \"dog\")\n",
    "- `sim(img, target)` = Cosine similarity to target class (e.g., \"cat\")\n",
    "\n",
    "Minimizing this makes image:\n",
    "- **Less dog-like** (in CLIP space)\n",
    "- **More cat-like** (in CLIP space)\n",
    "\n",
    "**Optimization:**\n",
    "- Gradient descent pushes image features toward target class\n",
    "- Perceptual loss keeps changes invisible to humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_image_features(img):\n",
    "    return clip_encoder.encode_image(img)\n",
    "\n",
    "@jit\n",
    "def nightshade_poisoning_loss(img, source_emb, target_emb):\n",
    "    img_features = compute_image_features(img)\n",
    "    sim_source = jnp.dot(img_features, source_emb)\n",
    "    sim_target = jnp.dot(img_features, target_emb)\n",
    "    return sim_source - sim_target\n",
    "\n",
    "print(\"Nightshade poisoning loss function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e028f3",
   "metadata": {},
   "source": [
    "## 11. Compiled PGD Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def nightshade_step_compiled(\n",
    "    current_img,\n",
    "    original_img,\n",
    "    source_emb,\n",
    "    target_emb,\n",
    "    variables,\n",
    "    edge_weight,\n",
    "    epsilon,\n",
    "    alpha,\n",
    "    perceptual_weight\n",
    "):\n",
    "    model = VisionTransformer()\n",
    "\n",
    "    def loss_fn(x):\n",
    "        resized = jax.image.resize(x, (CLIP_INPUT_SIZE, CLIP_INPUT_SIZE, 3), method='bilinear')\n",
    "        mean = jnp.array([0.48145466, 0.4578275, 0.40821073])\n",
    "        std = jnp.array([0.26862954, 0.26130258, 0.27577711])\n",
    "        normalized = (resized - mean) / std\n",
    "        features = model.apply(variables, normalized[None, ...])[0]\n",
    "        features = features / jnp.linalg.norm(features)\n",
    "\n",
    "        poison_loss = jnp.dot(features, source_emb) - jnp.dot(features, target_emb)\n",
    "        perceptual_loss = jnp.mean((x - original_img)**2 * (1.5 - edge_weight))\n",
    "\n",
    "        return poison_loss + perceptual_weight * perceptual_loss * 100\n",
    "\n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(current_img)\n",
    "    next_img = current_img - alpha * jnp.sign(grads * edge_weight)\n",
    "    delta = jnp.clip(next_img - original_img, -epsilon, epsilon)\n",
    "\n",
    "    return jnp.clip(original_img + delta, 0.0, 1.0), loss_val\n",
    "\n",
    "print(\"Compiled PGD step function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961027b2",
   "metadata": {},
   "source": [
    "## 12. Nightshade Protection Algorithm\n",
    "\n",
    "PGD (Projected Gradient Descent) with:\n",
    "- **Targeted misclassification loss**: Push to wrong class\n",
    "- **Perceptual loss**: Keep changes invisible to humans\n",
    "- **Edge-aware perturbation**: Concentrate changes in textured areas\n",
    "\n",
    "**Process:**\n",
    "1. Compute loss gradient (direction to mislead AI)\n",
    "2. Take small step in that direction\n",
    "3. Clip perturbation to ±epsilon (intensity limit)\n",
    "4. Repeat for many iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba782a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightshade_protect_image(\n",
    "    img,\n",
    "    source_class_name,\n",
    "    target_class_name,\n",
    "    intensity,\n",
    "    iterations,\n",
    "    perceptual_weight=1.5\n",
    "):\n",
    "    source_emb = generic_emb\n",
    "    target_emb = target_embeddings[target_class_name]\n",
    "\n",
    "    epsilon = intensity\n",
    "    alpha = epsilon / iterations * ALPHA_MULTIPLIER\n",
    "\n",
    "    print(f\"\\nApplying Nightshade poisoning...\")\n",
    "    print(f\"Source (real): Generic image\")\n",
    "    print(f\"Target (AI sees): {target_class_name}\")\n",
    "    print(f\"Epsilon: {epsilon:.4f} ({int(epsilon * 255)}/255 per channel)\")\n",
    "    print(f\"Alpha: {alpha:.6f}\")\n",
    "    print(f\"Iterations: {iterations}\")\n",
    "    print(f\"Perceptual weight: {perceptual_weight}\")\n",
    "    print(f\"\\nComputing edge mask...\")\n",
    "\n",
    "    edge_weight = get_edge_mask(img)\n",
    "\n",
    "    variables = clip_encoder.variables\n",
    "\n",
    "    print(f\"Compiling JIT (first run only)...\")\n",
    "\n",
    "    current_img = img\n",
    "    current_img, initial_loss = nightshade_step_compiled(\n",
    "        current_img, img, source_emb, target_emb,\n",
    "        variables, edge_weight, epsilon, alpha, perceptual_weight\n",
    "    )\n",
    "    print(f\"JIT compilation complete!\")\n",
    "    print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "\n",
    "    losses = [float(initial_loss)]\n",
    "    for i in tqdm(range(1, iterations), desc=\"Poisoning\", unit=\"iter\"):\n",
    "        current_img, loss_val = nightshade_step_compiled(\n",
    "            current_img, img, source_emb, target_emb,\n",
    "            variables, edge_weight, epsilon, alpha, perceptual_weight\n",
    "        )\n",
    "        losses.append(float(loss_val))\n",
    "\n",
    "    perturbation = current_img - img\n",
    "    max_change = float(jnp.max(jnp.abs(perturbation)))\n",
    "    avg_change = float(jnp.mean(jnp.abs(perturbation)))\n",
    "\n",
    "    print(f\"\\nNightshade poisoning complete!\")\n",
    "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Loss improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"Max pixel change: {max_change:.4f} ({int(max_change * 255)}/255)\")\n",
    "    print(f\"Avg pixel change: {avg_change:.4f} ({int(avg_change * 255)}/255)\")\n",
    "    print(f\"\\nThis image will poison AI models to classify it as '{target_class_name}'\")\n",
    "\n",
    "    return current_img\n",
    "\n",
    "print(\"Nightshade protection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad86878",
   "metadata": {},
   "source": [
    "## 13. Upload Test Image\n",
    "\n",
    "Upload an image you want to poison.\n",
    "\n",
    "**Example use cases:**\n",
    "- Upload dog photo, poison as \"Cat\" → AI learns dogs are cats\n",
    "- Upload artwork, poison as \"Abstract\" → AI misclassifies your style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b75e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Upload the image to poison:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    test_image_path = list(uploaded.keys())[0]\n",
    "    try:\n",
    "        original_img = load_image(test_image_path)\n",
    "        print(f\"Uploaded: {test_image_path}\")\n",
    "        print(f\"Size: {original_img.shape}\")\n",
    "        print(f\"Range: [{original_img.min():.3f}, {original_img.max():.3f}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        original_img = None\n",
    "else:\n",
    "    print(\"No file uploaded\")\n",
    "    original_img = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f207e",
   "metadata": {},
   "source": [
    "## 14. Run Nightshade Poisoning\n",
    "\n",
    "Choose target class from: `Dog`, `Cat`, `Car`, `Landscape`, `Person`, `Building`, `Food`, `Abstract`\n",
    "\n",
    "**Example scenarios:**\n",
    "- Real: Dog photo → Target: `Cat` → AI learns dog as cat\n",
    "- Real: Person photo → Target: `Car` → AI learns person as car\n",
    "- Real: Landscape → Target: `Abstract` → AI misclassifies landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16222a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_img is None:\n",
    "    print(\"No image loaded. Run Step 12 first.\")\n",
    "else:\n",
    "    # Options: 'Dog', 'Cat', 'Car', 'Landscape', 'Person', 'Building', 'Food', 'Abstract'\n",
    "    TARGET_CLASS = 'Cat'\n",
    "\n",
    "    print(f\"Available target classes: {list(target_embeddings.keys())}\")\n",
    "    print(f\"Selected target: {TARGET_CLASS}\")\n",
    "\n",
    "    try:\n",
    "        poisoned_img = nightshade_protect_image(\n",
    "            original_img,\n",
    "            source_class_name='Generic',\n",
    "            target_class_name=TARGET_CLASS,\n",
    "            intensity=INTENSITY,\n",
    "            iterations=ITERATIONS,\n",
    "            perceptual_weight=PERCEPTUAL_WEIGHT\n",
    "        )\n",
    "\n",
    "        output_path = f'/content/drive/MyDrive/hope-models/exports/nightshade_{TARGET_CLASS.lower()}.jpg'\n",
    "        save_image(poisoned_img, output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Poisoning failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526dca6",
   "metadata": {},
   "source": [
    "## 15. Display Results\n",
    "\n",
    "Compare original and poisoned images side-by-side.\n",
    "\n",
    "**What you should see:**\n",
    "- Images look nearly identical to human eyes\n",
    "- Max pixel change: 5-10 per channel (out of 255)\n",
    "- AI will classify poisoned image as target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_img is None:\n",
    "    print(\"No original image. Run Step 13 first.\")\n",
    "elif 'poisoned_img' not in dir() or poisoned_img is None:\n",
    "    print(\"No poisoned image. Run Step 14 first.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    axes[0].imshow(np.array(original_img))\n",
    "    axes[0].set_title('Original Image', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(np.array(poisoned_img))\n",
    "    axes[1].set_title(f'Poisoned (Target: {TARGET_CLASS})', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    diff = np.abs(np.array(poisoned_img) - np.array(original_img)) * 10\n",
    "    axes[2].imshow(diff)\n",
    "    axes[2].set_title('Difference (x10 amplified)', fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    max_diff = float(jnp.max(jnp.abs(poisoned_img - original_img)))\n",
    "    avg_diff = float(jnp.mean(jnp.abs(poisoned_img - original_img)))\n",
    "    print(f\"\\nChange statistics:\")\n",
    "    print(f\"  Max: {max_diff:.4f} ({int(max_diff * 255)}/255)\")\n",
    "    print(f\"  Avg: {avg_diff:.4f} ({int(avg_diff * 255)}/255)\")\n",
    "    print(f\"  Human visible: {'Yes' if max_diff > 0.05 else 'No (< 13/255)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15642c1",
   "metadata": {},
   "source": [
    "## 16. Download Poisoned Image\n",
    "\n",
    "Download the poisoned image for use in datasets.\n",
    "\n",
    "**Warning:** This image will cause AI models trained on it to learn incorrect associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8be898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "if 'TARGET_CLASS' not in dir():\n",
    "    print(\"TARGET_CLASS not defined. Run Step 14 first.\")\n",
    "else:\n",
    "    output_path = f'/content/drive/MyDrive/hope-models/exports/nightshade_{TARGET_CLASS.lower()}.jpg'\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Downloading: {output_path}\")\n",
    "        files.download(output_path)\n",
    "    else:\n",
    "        print(\"File not found. Run Step 14 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af4afc",
   "metadata": {},
   "source": [
    "## 17. Batch Protection Function\n",
    "\n",
    "Poison multiple images at once with same target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d85cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightshade_protect_batch(\n",
    "    image_paths,\n",
    "    output_dir,\n",
    "    target_class_name,\n",
    "    intensity=0.03,\n",
    "    iterations=500,\n",
    "    perceptual_weight=1.5\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {img_path}\")\n",
    "\n",
    "        img = load_image(img_path)\n",
    "\n",
    "        poisoned = nightshade_protect_image(\n",
    "            img,\n",
    "            source_class_name='Generic',\n",
    "            target_class_name=target_class_name,\n",
    "            intensity=intensity,\n",
    "            iterations=iterations,\n",
    "            perceptual_weight=perceptual_weight\n",
    "        )\n",
    "\n",
    "        basename = os.path.basename(img_path)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "        output_path = os.path.join(output_dir, f\"{name}_nightshade_{target_class_name.lower()}{ext}\")\n",
    "        save_image(poisoned, output_path)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batch poisoning complete!\")\n",
    "\n",
    "print(\"Batch protection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc246479",
   "metadata": {},
   "source": [
    "## 18. Export Model Parameters\n",
    "\n",
    "Save model data for ONNX/TFLite conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nightshade_model_for_export():\n",
    "    export_data = {\n",
    "        'vit_weights': clip_data['base_weights'],\n",
    "\n",
    "        'presets': {\n",
    "            'target_embeddings': {name: np.array(emb) for name, emb in target_embeddings.items()},\n",
    "            'generic_emb': np.array(generic_emb),\n",
    "        },\n",
    "\n",
    "        'target_prompts': NIGHTSHADE_TARGET_PROMPTS,\n",
    "        'generic_prompt': NIGHTSHADE_GENERIC_PROMPT,\n",
    "\n",
    "        'constants': {\n",
    "            'clip_mean': CLIP_MEAN,\n",
    "            'clip_std': CLIP_STD,\n",
    "            'clip_input_size': CLIP_INPUT_SIZE,\n",
    "        },\n",
    "\n",
    "        'parameter_presets': PARAMETER_PRESETS,\n",
    "\n",
    "        'default_params': {\n",
    "            'intensity': INTENSITY,\n",
    "            'iterations': ITERATIONS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'perceptual_weight': PERCEPTUAL_WEIGHT,\n",
    "            'alpha_multiplier': ALPHA_MULTIPLIER,\n",
    "        },\n",
    "\n",
    "        'architecture': {\n",
    "            'model_type': 'ViT-B/32',\n",
    "            'width': 768,\n",
    "            'layers': 12,\n",
    "            'heads': 12,\n",
    "            'patch_size': 32,\n",
    "            'output_dim': 512,\n",
    "        },\n",
    "\n",
    "        'algorithm': {\n",
    "            'name': 'Nightshade',\n",
    "            'type': 'data_poisoning',\n",
    "            'description': 'Targeted misclassification for AI training data poisoning',\n",
    "            'available_targets': list(NIGHTSHADE_TARGET_PROMPTS.keys()),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    export_path = '/content/drive/MyDrive/hope-models/exports/nightshade_model_export.pkl'\n",
    "\n",
    "    with open(export_path, 'wb') as f:\n",
    "        pickle.dump(export_data, f)\n",
    "\n",
    "    file_size = os.path.getsize(export_path) / (1024**2)\n",
    "    print(f\"Nightshade model export data saved\")\n",
    "    print(f\"Path: {export_path}\")\n",
    "    print(f\"Size: {file_size:.2f} MB\")\n",
    "    print(f\"Available targets: {list(NIGHTSHADE_TARGET_PROMPTS.keys())}\")\n",
    "    print(f\"Parameter presets: {list(PARAMETER_PRESETS.keys())}\")\n",
    "\n",
    "    return export_path\n",
    "\n",
    "export_path = save_nightshade_model_for_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c65e03",
   "metadata": {},
   "source": [
    "## 19. Model Information\n",
    "\n",
    "Display Nightshade model capabilities and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nightshade_model_info():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NIGHTSHADE POISONING MODEL INFO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nArchitecture:\")\n",
    "    print(f\"  Model: ViT-B/32\")\n",
    "    print(f\"  Parameters: ~150M\")\n",
    "    print(f\"  Input size: {CLIP_INPUT_SIZE}x{CLIP_INPUT_SIZE}\")\n",
    "    print(f\"  Feature dim: 512\")\n",
    "\n",
    "    print(\"\\nTarget Embeddings:\")\n",
    "    for name, emb in target_embeddings.items():\n",
    "        print(f\"  {name}: {emb.shape}\")\n",
    "    print(f\"  Generic (source): {generic_emb.shape}\")\n",
    "\n",
    "    print(\"\\nDefault Parameters:\")\n",
    "    print(f\"  Intensity: {INTENSITY} ({int(INTENSITY * 255)}/255)\")\n",
    "    print(f\"  Iterations: {ITERATIONS}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Perceptual weight: {PERCEPTUAL_WEIGHT}\")\n",
    "\n",
    "    print(\"\\nParameter Presets:\")\n",
    "    for name, preset in PARAMETER_PRESETS.items():\n",
    "        marker = \">\" if name == ACTIVE_PRESET else \" \"\n",
    "        print(f\"  {marker} {name}: {preset['description']}\")\n",
    "\n",
    "    print(\"\\nCapabilities:\")\n",
    "    print(f\"  Single image poisoning\")\n",
    "    print(f\"  Batch processing\")\n",
    "    print(f\"  8 target classes\")\n",
    "    print(f\"  Imperceptible to humans (< 10/255 change)\")\n",
    "    print(f\"  GPU acceleration (JAX)\")\n",
    "    print(f\"  JIT compilation\")\n",
    "\n",
    "    print(\"\\nExport Ready:\")\n",
    "    print(f\"  ONNX export compatible\")\n",
    "    print(f\"  TFLite export compatible\")\n",
    "    print(f\"  Preset embeddings saved\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print_nightshade_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a61b5",
   "metadata": {},
   "source": [
    "## Nightshade Algorithm Complete\n",
    "\n",
    "**Implemented:**\n",
    "- JAX-based data poisoning\n",
    "- CLIP-guided targeted misclassification\n",
    "- PGD optimization with perceptual loss\n",
    "- Edge-aware adaptive perturbation\n",
    "- 8 target classes\n",
    "\n",
    "**Parameters (Optimized for Invisibility + Effectiveness):**\n",
    "- Intensity: 0.03 (7/255 per channel - imperceptible)\n",
    "- Iterations: 500 (subtle changes)\n",
    "- Perceptual weight: 1.5 (strong invisibility)\n",
    "\n",
    "**Target Classes:**\n",
    "- Dog, Cat, Car, Landscape, Person, Building, Food, Abstract\n",
    "\n",
    "**How it works:**\n",
    "1. Human sees: **Normal image**\n",
    "2. AI learns: **Wrong class** (e.g., dog → cat)\n",
    "3. Result: **Poisoned training data**\n",
    "\n",
    "**Saved to:** `/content/drive/MyDrive/hope-models/exports/`\n",
    "\n",
    "**Exported:** `nightshade_model_export.pkl`\n",
    "\n",
    "**Next:** Run `5_export_onnx.ipynb` to convert to ONNX/TFLite for Hope Tauri app"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
