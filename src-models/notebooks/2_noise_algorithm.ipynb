{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea7babf",
   "metadata": {},
   "source": [
    "# Adversarial Noise Protection Algorithm\n",
    "\n",
    "Implements CLIP-guided adversarial perturbations in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740ccad",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779787f",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q jax[cuda12] jaxlib\n",
    "%pip install -q flax optax\n",
    "%pip install -q pillow numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e6e08",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "folders = [\n",
    "    '/content/drive/MyDrive/hope-models/checkpoints',\n",
    "    '/content/drive/MyDrive/hope-models/exports'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(\"Drive mounted and folders ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1474234",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d25c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import grad, jit, random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43581388",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578573f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return jnp.array(img) / 255.0\n",
    "\n",
    "def save_image(img_array, path):\n",
    "    img_array = np.clip(np.array(img_array) * 255, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img_array).save(path, quality=95)\n",
    "\n",
    "    print(f\"Saved: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffb587",
   "metadata": {},
   "source": [
    "## 5. Load CLIP Data\n",
    "\n",
    "Load pre-extracted weights and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d638e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/content/drive/MyDrive/hope-models/checkpoints/clip_data.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    clip_data = pickle.load(f)\n",
    "\n",
    "CLIP_MEAN = clip_data['clip_mean']\n",
    "CLIP_STD = clip_data['clip_std']\n",
    "CLIP_INPUT_SIZE = clip_data['clip_input_size']\n",
    "\n",
    "chaos_embeddings = jnp.array(clip_data['chaos_embeddings_base'])\n",
    "normal_embeddings = jnp.array(clip_data['normal_embeddings_base'])\n",
    "\n",
    "print(f\"Loaded CLIP data\")\n",
    "print(f\"Mean: {CLIP_MEAN}\")\n",
    "print(f\"Std: {CLIP_STD}\")\n",
    "print(f\"Input size: {CLIP_INPUT_SIZE}\")\n",
    "print(f\"Chaos embeddings: {chaos_embeddings.shape}\")\n",
    "print(f\"Normal embeddings: {normal_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b9e73",
   "metadata": {},
   "source": [
    "## 6. Base Algorithm Parameters\n",
    "\n",
    "Based on original Hope implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f893bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENSITY = 0.06\n",
    "ITERATIONS = 200\n",
    "LEARNING_RATE = 0.01\n",
    "PERCEPTUAL_WEIGHT = 0.5\n",
    "\n",
    "print(f\"Intensity: {INTENSITY} ({int(INTENSITY * 255)}/255 per channel)\")\n",
    "print(f\"Iterations: {ITERATIONS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Perceptual weight: {PERCEPTUAL_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965309",
   "metadata": {},
   "source": [
    "## 7. Define JAX/Flax ViT Architecture\n",
    "\n",
    "Flax/Linen implementation of CLIP's Vision Transformer (ViT-B/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3535d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class CLIPAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        d_model = x.shape[-1]\n",
    "        qkv = nn.Dense(3 * d_model, name=\"in_proj\")(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "\n",
    "        def split_heads(t):\n",
    "            return t.reshape(t.shape[0], t.shape[1], self.num_heads, -1).transpose(0, 2, 1, 3)\n",
    "\n",
    "        q, k, v = split_heads(q), split_heads(k), split_heads(v)\n",
    "        scale = (d_model // self.num_heads) ** -0.5\n",
    "        attn_weights = jax.nn.softmax((q @ k.transpose(0, 1, 3, 2)) * scale, axis=-1)\n",
    "        out = (attn_weights @ v).transpose(0, 2, 1, 3).reshape(x.shape[0], x.shape[1], -1)\n",
    "        return nn.Dense(d_model, name=\"out_proj\")(out)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    width: int\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.width * 4, name=\"c_fc\")(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(self.width, name=\"c_proj\")(x)\n",
    "        return x\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    num_heads: int\n",
    "    width: int\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x + CLIPAttention(self.num_heads, name=\"attn\")(nn.LayerNorm(name=\"ln_1\")(x))\n",
    "        x = x + MLP(self.width, name=\"mlp\")(nn.LayerNorm(name=\"ln_2\")(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    width: int = 768\n",
    "    layers: int = 12\n",
    "    heads: int = 12\n",
    "    patch_size: int = 32\n",
    "    output_dim: int = 512\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(self.width, (self.patch_size, self.patch_size),\n",
    "                    strides=(self.patch_size, self.patch_size),\n",
    "                    padding='VALID', use_bias=False, name=\"conv1\")(x)\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[-1])\n",
    "        cls_token = self.param('class_embedding', lambda *args: jnp.zeros((self.width,)))\n",
    "        pos_embed = self.param('positional_embedding', lambda *args: jnp.zeros((50, self.width)))\n",
    "        x = jnp.concatenate([jnp.broadcast_to(cls_token, (x.shape[0], 1, self.width)), x], axis=1)\n",
    "        x = x + pos_embed\n",
    "        x = nn.LayerNorm(name=\"ln_pre\")(x)\n",
    "        for i in range(self.layers):\n",
    "            x = ResidualAttentionBlock(self.heads, self.width, name=f\"transformer_resblocks_{i}\")(x)\n",
    "        x = nn.LayerNorm(name=\"ln_post\")(x[:, 0, :])\n",
    "        return nn.Dense(self.output_dim, use_bias=False, name=\"proj\")(x)\n",
    "\n",
    "print(\"ViT architecture defined (CLIPAttention, MLP, ResidualAttentionBlock, VisionTransformer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b933dd2",
   "metadata": {},
   "source": [
    "## 8. Weight Conversion Function\n",
    "\n",
    "Convert PyTorch CLIP weights to Flax parameter format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f843a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clip_weights(pt_weights):\n",
    "    flax_params = {}\n",
    "\n",
    "    for key, value in pt_weights.items():\n",
    "        if key.startswith('visual.'):\n",
    "            key = key[7:]\n",
    "\n",
    "        value = jnp.array(value)\n",
    "\n",
    "        if key == 'class_embedding':\n",
    "            flax_params['class_embedding'] = value\n",
    "            continue\n",
    "\n",
    "        elif key == 'positional_embedding':\n",
    "            flax_params['positional_embedding'] = value\n",
    "            continue\n",
    "\n",
    "        elif key == 'proj':\n",
    "            if value.shape == (512, 768):\n",
    "                flax_params['proj/kernel'] = value.T\n",
    "            elif value.shape == (768, 512):\n",
    "                flax_params['proj/kernel'] = value\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected proj shape: {value.shape}\")\n",
    "            print(f\"proj: input shape {pt_weights['proj'].shape} -> output shape {flax_params['proj/kernel'].shape}\")\n",
    "            continue\n",
    "\n",
    "        elif key == 'conv1.weight':\n",
    "            flax_params['conv1/kernel'] = jnp.transpose(value, (2, 3, 1, 0))\n",
    "            continue\n",
    "\n",
    "        key = key.replace('transformer.resblocks.', 'transformer_resblocks_').replace('.', '/')\n",
    "\n",
    "        if 'in_proj_weight' in key:\n",
    "            flax_params[key.replace('in_proj_weight', 'in_proj/kernel')] = value.T\n",
    "\n",
    "        elif 'in_proj_bias' in key:\n",
    "            flax_params[key.replace('in_proj_bias', 'in_proj/bias')] = value\n",
    "\n",
    "        elif 'weight' in key and 'ln' in key:\n",
    "            flax_params[key.replace('weight', 'scale')] = value\n",
    "\n",
    "        elif 'bias' in key and 'ln' in key:\n",
    "            flax_params[key] = value\n",
    "\n",
    "        elif 'weight' in key:\n",
    "            flax_params[key.replace('weight', 'kernel')] = value.T\n",
    "\n",
    "        else:\n",
    "            flax_params[key] = value\n",
    "\n",
    "    nested_params = {}\n",
    "    for key, value in flax_params.items():\n",
    "        parts = key.split('/')\n",
    "        curr = nested_params\n",
    "        for p in parts[:-1]:\n",
    "            curr = curr.setdefault(p, {})\n",
    "        curr[parts[-1]] = value\n",
    "\n",
    "    return {'params': nested_params}\n",
    "\n",
    "print(\"Weight conversion function defined (convert_clip_weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923e3c9",
   "metadata": {},
   "source": [
    "## 9. Create Reusable CLIP Encoder\n",
    "\n",
    "Encoder class for use across all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class CLIPEncoder:\n",
    "    def __init__(self, weights):\n",
    "        self.model_vit = VisionTransformer()\n",
    "        self.variables = convert_clip_weights(weights)\n",
    "\n",
    "        print(\"CLIP encoder initialized\")\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def encode_image(self, img):\n",
    "        img_resized = jax.image.resize(img, (CLIP_INPUT_SIZE, CLIP_INPUT_SIZE, 3), method='bilinear')\n",
    "        mean = jnp.array([0.48145466, 0.4578275, 0.40821073])\n",
    "        std = jnp.array([0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "        normalized = (img_resized - mean) / std\n",
    "        features = self.model_vit.apply(self.variables, normalized[None, ...])\n",
    "\n",
    "        return features[0] / jnp.linalg.norm(features[0])\n",
    "\n",
    "clip_encoder = CLIPEncoder(clip_data['base_weights'])\n",
    "print(\"Global CLIP encoder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a209b",
   "metadata": {},
   "source": [
    "## 10. CLIP Image Encoding (Simplified)\n",
    "\n",
    "Simplified CLIP encoding using pre-computed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vit = VisionTransformer()\n",
    "variables = convert_clip_weights(clip_data['base_weights'])\n",
    "\n",
    "@jit\n",
    "def compute_image_features(img):\n",
    "    return clip_encoder.encode_image(img)\n",
    "\n",
    "@jit\n",
    "def semantic_loss_noise(img, chaos_emb, normal_emb):\n",
    "    img_features = compute_image_features(img)\n",
    "    sim_chaos = jnp.mean(jnp.dot(chaos_emb, img_features))\n",
    "    sim_normal = jnp.mean(jnp.dot(normal_emb, img_features))\n",
    "\n",
    "    return -sim_chaos + sim_normal\n",
    "\n",
    "semantic_loss = semantic_loss_noise\n",
    "\n",
    "print(\"Noise loss function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b23ce2",
   "metadata": {},
   "source": [
    "## 11. Protection Algorithm\n",
    "\n",
    "PGD-based adversarial perturbation with perceptual optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f23385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def protect_image(img, intensity, iterations, chaos_emb, normal_emb, perceptual_weight=0.5):\n",
    "    epsilon = intensity\n",
    "    alpha = epsilon / iterations * 2.5\n",
    "\n",
    "    print(f\"\\nProtecting image...\")\n",
    "    print(f\"Epsilon: {epsilon:.4f} ({int(epsilon * 255)}/255 per channel)\")\n",
    "    print(f\"Alpha: {alpha:.6f}\")\n",
    "    print(f\"Iterations: {iterations}\")\n",
    "    print(f\"Perceptual weight: {perceptual_weight}\")\n",
    "    print(f\"\\nCompiling JIT (this may take a few minutes on first run)...\")\n",
    "\n",
    "    def compute_edge_weight(image):\n",
    "        gray = jnp.mean(image, axis=-1)\n",
    "\n",
    "        gx = jnp.abs(gray[1:, :] - gray[:-1, :])\n",
    "        gy = jnp.abs(gray[:, 1:] - gray[:, :-1])\n",
    "\n",
    "        gx = jnp.pad(gx, ((0, 1), (0, 0)), mode='edge')\n",
    "        gy = jnp.pad(gy, ((0, 0), (0, 1)), mode='edge')\n",
    "\n",
    "        edges = jnp.sqrt(gx**2 + gy**2)\n",
    "        edges = (edges - edges.min()) / (edges.max() - edges.min() + 1e-8)\n",
    "\n",
    "        weight = 0.3 + 0.7 * edges\n",
    "        return weight[..., None]\n",
    "\n",
    "    edge_weight = compute_edge_weight(img)\n",
    "\n",
    "    @jit\n",
    "    def compute_perceptual_loss(perturbed, original):\n",
    "        diff = perturbed - original\n",
    "        smooth_penalty = jnp.mean(diff**2 * (1.5 - edge_weight))\n",
    "        return smooth_penalty\n",
    "\n",
    "    @jit\n",
    "    def combined_loss(current_img, chaos_emb, normal_emb, original_img):\n",
    "        semantic = semantic_loss(current_img, chaos_emb, normal_emb)\n",
    "        perceptual = compute_perceptual_loss(current_img, original_img)\n",
    "        return semantic + perceptual_weight * perceptual * 100\n",
    "\n",
    "    @jit\n",
    "    def pgd_step(current_img, original_img):\n",
    "        loss_val, grads = jax.value_and_grad(combined_loss)(\n",
    "            current_img, chaos_emb, normal_emb, original_img\n",
    "        )\n",
    "\n",
    "        weighted_grads = grads * edge_weight\n",
    "\n",
    "        next_img = current_img - alpha * jnp.sign(weighted_grads)\n",
    "        delta = jnp.clip(next_img - original_img, -epsilon, epsilon)\n",
    "        next_img = jnp.clip(original_img + delta, 0.0, 1.0)\n",
    "\n",
    "        return next_img, loss_val\n",
    "\n",
    "    current_img = img\n",
    "    current_img, initial_loss = pgd_step(current_img, img)\n",
    "    print(f\"JIT compilation complete!\")\n",
    "    print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "\n",
    "    losses = [float(initial_loss)]\n",
    "    for i in tqdm(range(1, iterations), desc=\"Optimizing\", unit=\"iter\"):\n",
    "        current_img, loss_val = pgd_step(current_img, img)\n",
    "        losses.append(float(loss_val))\n",
    "\n",
    "    perturbation = current_img - img\n",
    "    max_change = float(jnp.max(jnp.abs(perturbation)))\n",
    "    avg_change = float(jnp.mean(jnp.abs(perturbation)))\n",
    "\n",
    "    print(f\"\\nProtection complete!\")\n",
    "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Loss improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "    print(f\"Max pixel change: {max_change:.4f} ({int(max_change * 255)}/255)\")\n",
    "    print(f\"Avg pixel change: {avg_change:.4f} ({int(avg_change * 255)}/255)\")\n",
    "\n",
    "    return current_img\n",
    "\n",
    "print(\"Protection function defined (PGD/Noise with perceptual optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87d7bc",
   "metadata": {},
   "source": [
    "## 12. Upload image\n",
    "\n",
    "Upload a test image and apply protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6632aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Upload the image sample:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    test_image_path = list(uploaded.keys())[0]\n",
    "    try:\n",
    "        original_img = load_image(test_image_path)\n",
    "\n",
    "        print(f\"Uploaded: {test_image_path}\")\n",
    "        print(f\"Size: {original_img.shape}\")\n",
    "        print(f\"Range: [{original_img.min():.3f}, {original_img.max():.3f}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        original_img = None\n",
    "else:\n",
    "    print(\"No file uploaded\")\n",
    "    original_img = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4b5ad",
   "metadata": {},
   "source": [
    "## 13. Run Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a072ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_img is None:\n",
    "    print(\"No image loaded. Please run Step 10.1 first.\")\n",
    "else:\n",
    "    print(\"Starting image protection (PGD/Noise Algorithm with perceptual loss)...\")\n",
    "\n",
    "    try:\n",
    "        protected_img = protect_image(\n",
    "            original_img,\n",
    "            intensity=INTENSITY,\n",
    "            iterations=ITERATIONS,\n",
    "            chaos_emb=chaos_embeddings,\n",
    "            normal_emb=normal_embeddings,\n",
    "            perceptual_weight=PERCEPTUAL_WEIGHT\n",
    "        )\n",
    "\n",
    "        output_path = '/content/drive/MyDrive/hope-models/exports/protected_test.jpg'\n",
    "        save_image(protected_img, output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Protection failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93f0fb",
   "metadata": {},
   "source": [
    "## 14. Displaying and Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "if original_img is None or protected_img is None:\n",
    "    print(\"No images to display. Run steps 10.1 and 10.2 first.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    axes[0].imshow(np.array(original_img))\n",
    "    axes[0].set_title('Original Image', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(np.array(protected_img))\n",
    "    axes[1].set_title(f'Protected (Intensity: {INTENSITY})', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Display complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbb681",
   "metadata": {},
   "source": [
    "## 15. Download results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "output_path = '/content/drive/MyDrive/hope-models/exports/protected_test.jpg'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Preparing to download the file: {output_path}\")\n",
    "    files.download(output_path)\n",
    "else:\n",
    "    print(\"Can't find file, run step 10 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17986063",
   "metadata": {},
   "source": [
    "## 16. Batch Protection Function\n",
    "\n",
    "Protect multiple images at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protect_batch(image_paths, output_dir, intensity=0.30, iterations=150):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {img_path}\")\n",
    "\n",
    "        img = load_image(img_path)\n",
    "\n",
    "        protected = protect_image(\n",
    "            img, intensity, iterations,\n",
    "            chaos_embeddings, normal_embeddings\n",
    "        )\n",
    "\n",
    "        basename = os.path.basename(img_path)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "        output_path = os.path.join(output_dir, f\"{name}_protected{ext}\")\n",
    "        save_image(protected, output_path)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batch protection complete!\")\n",
    "\n",
    "print(\"Batch protection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efcc0c",
   "metadata": {},
   "source": [
    "## 17. Export Model Parameters\n",
    "\n",
    "Save model in format compatible with ONNX/TFLite export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ffd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_export():\n",
    "    export_data = {\n",
    "        'vit_weights': clip_data['base_weights'],\n",
    "\n",
    "        'presets': {\n",
    "            'noise_chaos': chaos_embeddings,\n",
    "            'noise_normal': normal_embeddings,\n",
    "        },\n",
    "\n",
    "        'constants': {\n",
    "            'clip_mean': CLIP_MEAN,\n",
    "            'clip_std': CLIP_STD,\n",
    "            'clip_input_size': CLIP_INPUT_SIZE,\n",
    "        },\n",
    "\n",
    "        'default_params': {\n",
    "            'intensity': INTENSITY,\n",
    "            'iterations': ITERATIONS,\n",
    "            'alpha_multiplier': 2.0,\n",
    "        },\n",
    "\n",
    "        'architecture': {\n",
    "            'model_type': 'ViT-B/32',\n",
    "            'width': 768,\n",
    "            'layers': 12,\n",
    "            'heads': 12,\n",
    "            'patch_size': 32,\n",
    "            'output_dim': 512,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    export_path = '/content/drive/MyDrive/hope-models/exports/noise_model_export.pkl'\n",
    "\n",
    "    with open(export_path, 'wb') as f:\n",
    "        pickle.dump(export_data, f)\n",
    "\n",
    "    file_size = os.path.getsize(export_path) / (1024**2)\n",
    "    print(f\"Model export data saved\")\n",
    "    print(f\"Path: {export_path}\")\n",
    "    print(f\"Size: {file_size:.2f} MB\")\n",
    "    print(f\"Presets: {len(export_data['presets'])}\")\n",
    "\n",
    "    return export_path\n",
    "\n",
    "export_path = save_model_for_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd4b89",
   "metadata": {},
   "source": [
    "## 18. Model Information\n",
    "\n",
    "Display model statistics and capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info():\n",
    "    print(\"=\"*60)\n",
    "    print(\"NOISE PROTECTION MODEL INFO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"\\nArchitecture:\")\n",
    "    print(f\"Model: ViT-B/32\")\n",
    "    print(f\"Parameters: ~150M\")\n",
    "    print(f\"Input size: {CLIP_INPUT_SIZE}x{CLIP_INPUT_SIZE}\")\n",
    "    print(f\"Feature dim: 512\")\n",
    "\n",
    "    print(\"\\nEmbeddings:\")\n",
    "    print(f\"Chaos prompts: {chaos_embeddings.shape[0]}\")\n",
    "    print(f\"Normal prompts: {normal_embeddings.shape[0]}\")\n",
    "    print(f\"Embedding dim: {chaos_embeddings.shape[1]}\")\n",
    "\n",
    "    print(\"\\nDefault Parameters:\")\n",
    "    print(f\"Intensity: {INTENSITY}\")\n",
    "    print(f\"Iterations: {ITERATIONS}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "    print(\"\\nCapabilities:\")\n",
    "    print(f\"Single image protection\")\n",
    "    print(f\"Batch processing\")\n",
    "    print(f\"Adjustable intensity\")\n",
    "    print(f\"GPU acceleration (JAX)\")\n",
    "    print(f\"JIT compilation\")\n",
    "\n",
    "    print(\"\\nExport Ready:\")\n",
    "    print(f\"ONNX export compatible\")\n",
    "    print(f\"TFLite export compatible\")\n",
    "    print(f\"Preset embeddings saved\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bca6c",
   "metadata": {},
   "source": [
    "## Algorithm Complete\n",
    "\n",
    "**Implemented:**\n",
    "- JAX-based adversarial perturbations\n",
    "- CLIP-guided semantic loss\n",
    "- PGD optimization\n",
    "- Batch processing support\n",
    "\n",
    "**Parameters:**\n",
    "- Intensity: 0.06 (adjustable)\n",
    "- Iterations: 200 (adjustable)\n",
    "- Chaos prompts: 8 prompts\n",
    "- Normal prompts: 3 prompts\n",
    "\n",
    "**Saved to:** `/content/drive/MyDrive/hope-models/exports/`\n",
    "\n",
    "**Next:** Run `3_glaze_algorithm.ipynb` or `5_export_onnx.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
